{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Import useful modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/mfarhi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/mfarhi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package words to /Users/mfarhi/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import words\n",
    "from nltk.metrics.distance import edit_distance\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity, linear_kernel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('words')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Load and prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/processedPositive.txt') as f:\n",
    "    pos_data = f.read().splitlines()\n",
    "with open('data/processedNegative.txt') as f:\n",
    "    neg_data = f.read().splitlines()\n",
    "with open('data/processedNeutral.txt') as f:\n",
    "    neu_data = f.read().splitlines()\n",
    "\n",
    "processed_positive_df = pd.DataFrame({'tweets': pos_data, 'labels': 1})\n",
    "processed_negative_df = pd.DataFrame({'tweets': neg_data, 'labels': -1})\n",
    "processed_neutral_df = pd.DataFrame({'tweets': neu_data, 'labels': 0})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenate all three categories into one dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([processed_positive_df, processed_negative_df, processed_neutral_df], ignore_index=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.duplicated() == False]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset into 80% training and 20% test with stratification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['tweets'], df['labels'], test_size=0.2, stratify = df['labels'], random_state=1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure the split was stratified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0    0.373797\n",
       "-1    0.318653\n",
       " 1    0.307550\n",
       "Name: labels, dtype: float64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['labels'].value_counts() / len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0    0.373901\n",
       "-1    0.318371\n",
       " 1    0.307728\n",
       "Name: labels, dtype: float64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts() / len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0    0.373383\n",
       "-1    0.319778\n",
       " 1    0.306839\n",
       "Name: labels, dtype: float64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts() / len(y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter_stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StemVectorizer(CountVectorizer):\n",
    "    def build_tokenizer(self):\n",
    "        tokenizer = super().build_tokenizer()\n",
    "        return lambda doc: list(map(porter_stemmer.stem, tokenizer(doc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StemTfidfVectorizer(TfidfVectorizer):\n",
    "    def build_tokenizer(self):\n",
    "        tokenizer = super().build_tokenizer()\n",
    "        return lambda doc: list(map(porter_stemmer.stem, tokenizer(doc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LemmaVectorizer(CountVectorizer):\n",
    "    def build_tokenizer(self):\n",
    "        tokenizer = super().build_tokenizer()\n",
    "        return lambda doc: list(map(lemmatizer.lemmatize, tokenizer(doc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LemmaTfidfVectorizer(TfidfVectorizer):\n",
    "    def build_tokenizer(self):\n",
    "        tokenizer = super().build_tokenizer()\n",
    "        return lambda doc: list(map(lemmatizer.lemmatize, tokenizer(doc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "234377"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = {word.lower() for word in words.words()}\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def memoize(f):\n",
    "    cache = {}\n",
    "    def memoized_f(*args):\n",
    "        if args not in cache:\n",
    "            cache[args] = f(*args)\n",
    "        return cache[args]\n",
    "    return memoized_f\n",
    "\n",
    "@memoize\n",
    "def correct_word(word):\n",
    "    distances = [(w, edit_distance(word, w)) for w in vocab if (w[0] == word[0] and w[-1] == word[-1] and len(w) - len(word) in [-1, 0, 1])]\n",
    "    return min(distances, key=lambda x: x[1])[0] if distances else word\n",
    "\n",
    "def autocorrect(tokens):\n",
    "    return [word if (word in vocab or (not word.isalpha())) else correct_word(word) for word in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StemAutocorrectVectorizer(CountVectorizer):\n",
    "    def build_tokenizer(self):\n",
    "        tokenizer = super().build_tokenizer()\n",
    "        return lambda doc: autocorrect(list(map(porter_stemmer.stem, tokenizer(doc))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StemAutocorrectTfidfVectorizer(TfidfVectorizer):\n",
    "    def build_tokenizer(self):\n",
    "        tokenizer = super().build_tokenizer()\n",
    "        return lambda doc: autocorrect(list(map(porter_stemmer.stem, tokenizer(doc))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LemmaAutocorrectVectorizer(CountVectorizer):\n",
    "    def build_tokenizer(self):\n",
    "        tokenizer = super().build_tokenizer()\n",
    "        return lambda doc: autocorrect(list(map(lemmatizer.lemmatize, tokenizer(doc))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LemmaAutocorrectTfidfVectorizer(TfidfVectorizer):\n",
    "    def build_tokenizer(self):\n",
    "        tokenizer = super().build_tokenizer()\n",
    "        return lambda doc: autocorrect(list(map(lemmatizer.lemmatize, tokenizer(doc))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorization_techniques = ['binary', 'word_count', 'tfidf', 'bigrams']\n",
    "preprocessing_techniques = ['just_tokenization', 'stemming', 'lemmatization', 'stemming+misspellings', 'lemmatization+misspellings']\n",
    "## create dataframe to store results\n",
    "results = pd.DataFrame(columns=vectorization_techniques, index=preprocessing_techniques)\n",
    "for v in vectorization_techniques:\n",
    "    for p in preprocessing_techniques:\n",
    "        if v == 'word_count' or v == 'binary':\n",
    "            if p == 'just_tokenization':\n",
    "                vectorizer = CountVectorizer(binary=(v == 'binary'))\n",
    "            elif p == 'stemming':\n",
    "                vectorizer = StemVectorizer(binary=(v == 'binary'))\n",
    "            elif p == 'lemmatization':\n",
    "                vectorizer = LemmaVectorizer(binary=(v == 'binary'))\n",
    "            elif p == 'stemming+misspellings':\n",
    "                vectorizer = StemAutocorrectVectorizer(binary=(v == 'binary'))\n",
    "            elif p == 'lemmatization+misspellings':\n",
    "                vectorizer = LemmaAutocorrectVectorizer(binary=(v == 'binary'))\n",
    "        elif v == 'tfidf':\n",
    "            if p == 'just_tokenization':\n",
    "                vectorizer = TfidfVectorizer()\n",
    "            elif p == 'stemming':\n",
    "                vectorizer = StemTfidfVectorizer()\n",
    "            elif p == 'lemmatization':\n",
    "                vectorizer = LemmaTfidfVectorizer()\n",
    "            elif p == 'stemming+misspellings':\n",
    "                vectorizer = StemAutocorrectTfidfVectorizer()\n",
    "            elif p == 'lemmatization+misspellings':\n",
    "                vectorizer = LemmaAutocorrectTfidfVectorizer()\n",
    "        elif v == 'bigrams':\n",
    "            if p == 'just_tokenization':\n",
    "                vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "            elif p == 'stemming':\n",
    "                vectorizer = StemVectorizer(ngram_range=(1, 2))\n",
    "            elif p == 'lemmatization':\n",
    "                vectorizer = LemmaVectorizer(ngram_range=(1, 2))\n",
    "            elif p == 'stemming+misspellings':\n",
    "                vectorizer = StemAutocorrectVectorizer(ngram_range=(1, 2))\n",
    "            elif p == 'lemmatization+misspellings':\n",
    "                vectorizer = LemmaAutocorrectVectorizer(ngram_range=(1, 2))\n",
    "        vectorizer.fit(X_train)\n",
    "        results.loc[p, v] = vectorizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Similarity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use different datasets that you prepared in the task above and cosine similarity to find the top-10 similar pairs of tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_tweets(X_vec, normalized=False, n=10):\n",
    "    similarity = linear_kernel(X_vec, X_vec) if normalized else cosine_similarity(X_vec, X_vec)\n",
    "    similarity = pd.DataFrame(similarity)\n",
    "    similarity = similarity.stack().reset_index()\n",
    "    similarity.columns = ['tweet1', 'tweet2', 'similarity']\n",
    "    similarity = similarity[similarity['tweet1'] < similarity['tweet2']]\n",
    "    similarity = similarity.sort_values(by='similarity', ascending=False)\n",
    "    similarity = similarity[similarity.duplicated(subset='tweet1') == False]\n",
    "    similarity = similarity[similarity.duplicated(subset='tweet2') == False]\n",
    "    top = similarity.head(n)\n",
    "    return top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing: just_tokenization, Vectorization: binary\n",
      "Tweet 77: thank you! happy\n",
      "Tweet 546: thank you happy\n",
      "Similarity:  1.0000000000000002\n",
      "Tweet 1917: Thanks for being top engaged community members this week happy  Want this\n",
      "Tweet 2006: Thanks for being top engaged community members this week happy  Want this ?\n",
      "Similarity:  1.0000000000000002\n",
      "Tweet 1136: When will you notice me? unhappy\n",
      "Tweet 1934: when will you notice me? unhappy\n",
      "Similarity:  1.0000000000000002\n",
      "Tweet 546: thank you happy\n",
      "Tweet 1043: Thank you! happy\n",
      "Similarity:  1.0000000000000002\n",
      "Tweet 48: Thank you happy\n",
      "Tweet 77: thank you! happy\n",
      "Similarity:  1.0000000000000002\n",
      "Tweet 1469: miss you unhappy\n",
      "Tweet 1728: I miss you unhappy\n",
      "Similarity:  1.0000000000000002\n",
      "Tweet 1141: thanks for the recent follow. Much appreciated happy   Want this ?\n",
      "Tweet 1744: thanks for the recent follow, much appreciated happy   Want this ?\n",
      "Similarity:  0.9999999999999999\n",
      "Tweet 953: Hello everyone, have a great Thursday! Looking forward to reading your tweets happy  Want this\n",
      "Tweet 974: Hello everyone, have a great Thursday! Looking forward to reading your tweets happy   Want this\n",
      "Similarity:  0.9999999999999999\n",
      "Tweet 1407: thanks for the recent follow. Much appreciated happy   Want this\n",
      "Tweet 1962: thanks for the recent follow. Much appreciated happy  Want this ?\n",
      "Similarity:  0.9999999999999999\n",
      "Tweet 524: thanks for the recent follow. Much appreciated happy  Want this\n",
      "Tweet 1141: thanks for the recent follow. Much appreciated happy   Want this ?\n",
      "Similarity:  0.9999999999999999\n",
      "--------------------------------------------------\n",
      "Preprocessing: stemming, Vectorization: binary\n",
      "Tweet 77: thank you! happy\n",
      "Tweet 1043: Thank you! happy\n",
      "Similarity:  1.0000000000000002\n",
      "Tweet 1113: Thanks for being top engaged community members this week happy  (Want this ?\n",
      "Tweet 1917: Thanks for being top engaged community members this week happy  Want this\n",
      "Similarity:  1.0000000000000002\n",
      "Tweet 1469: miss you unhappy\n",
      "Tweet 1728: I miss you unhappy\n",
      "Similarity:  1.0000000000000002\n",
      "Tweet 1917: Thanks for being top engaged community members this week happy  Want this\n",
      "Tweet 2006: Thanks for being top engaged community members this week happy  Want this ?\n",
      "Similarity:  1.0000000000000002\n",
      "Tweet 1136: When will you notice me? unhappy\n",
      "Tweet 1934: when will you notice me? unhappy\n",
      "Similarity:  1.0000000000000002\n",
      "Tweet 1407: thanks for the recent follow. Much appreciated happy   Want this\n",
      "Tweet 1674: thanks for the recent follow. Much appreciated happy  Want this?\n",
      "Similarity:  0.9999999999999999\n",
      "Tweet 524: thanks for the recent follow. Much appreciated happy  Want this\n",
      "Tweet 2001: thanks for the recent follow, much appreciated happy   Want this\n",
      "Similarity:  0.9999999999999999\n",
      "Tweet 1674: thanks for the recent follow. Much appreciated happy  Want this?\n",
      "Tweet 1744: thanks for the recent follow, much appreciated happy   Want this ?\n",
      "Similarity:  0.9999999999999999\n",
      "Tweet 974: Hello everyone, have a great Thursday! Looking forward to reading your tweets happy   Want this\n",
      "Tweet 1492: Hello everyone, have a great Thursday! Looking forward to reading your tweets happy  Want this ?\n",
      "Similarity:  0.9999999999999999\n",
      "Tweet 1429: Thanks for the recent follow Happy to connect happy  have a great Thursday. (Want this?\n",
      "Tweet 1663: Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this?\n",
      "Similarity:  0.9999999999999998\n",
      "--------------------------------------------------\n",
      "Preprocessing: lemmatization, Vectorization: binary\n",
      "Tweet 77: thank you! happy\n",
      "Tweet 366: THANK YOU happy\n",
      "Similarity:  1.0000000000000002\n",
      "Tweet 546: thank you happy\n",
      "Tweet 1043: Thank you! happy\n",
      "Similarity:  1.0000000000000002\n",
      "Tweet 366: THANK YOU happy\n",
      "Tweet 546: thank you happy\n",
      "Similarity:  1.0000000000000002\n",
      "Tweet 1917: Thanks for being top engaged community members this week happy  Want this\n",
      "Tweet 2006: Thanks for being top engaged community members this week happy  Want this ?\n",
      "Similarity:  1.0000000000000002\n",
      "Tweet 1469: miss you unhappy\n",
      "Tweet 1728: I miss you unhappy\n",
      "Similarity:  1.0000000000000002\n",
      "Tweet 423: Thanks for being top engaged community members this week happy   Want this\n",
      "Tweet 1113: Thanks for being top engaged community members this week happy  (Want this ?\n",
      "Similarity:  1.0000000000000002\n",
      "Tweet 1136: When will you notice me? unhappy\n",
      "Tweet 1934: when will you notice me? unhappy\n",
      "Similarity:  1.0000000000000002\n",
      "Tweet 1141: thanks for the recent follow. Much appreciated happy   Want this ?\n",
      "Tweet 1962: thanks for the recent follow. Much appreciated happy  Want this ?\n",
      "Similarity:  0.9999999999999999\n",
      "Tweet 953: Hello everyone, have a great Thursday! Looking forward to reading your tweets happy  Want this\n",
      "Tweet 974: Hello everyone, have a great Thursday! Looking forward to reading your tweets happy   Want this\n",
      "Similarity:  0.9999999999999999\n",
      "Tweet 1962: thanks for the recent follow. Much appreciated happy  Want this ?\n",
      "Tweet 2001: thanks for the recent follow, much appreciated happy   Want this\n",
      "Similarity:  0.9999999999999999\n",
      "--------------------------------------------------\n",
      "Preprocessing: stemming+misspellings, Vectorization: binary\n",
      "Tweet 423: Thanks for being top engaged community members this week happy   Want this\n",
      "Tweet 1113: Thanks for being top engaged community members this week happy  (Want this ?\n",
      "Similarity:  1.0000000000000002\n",
      "Tweet 1136: When will you notice me? unhappy\n",
      "Tweet 1934: when will you notice me? unhappy\n",
      "Similarity:  1.0000000000000002\n",
      "Tweet 77: thank you! happy\n",
      "Tweet 1043: Thank you! happy\n",
      "Similarity:  1.0000000000000002\n",
      "Tweet 48: Thank you happy\n",
      "Tweet 366: THANK YOU happy\n",
      "Similarity:  1.0000000000000002\n",
      "Tweet 1113: Thanks for being top engaged community members this week happy  (Want this ?\n",
      "Tweet 1917: Thanks for being top engaged community members this week happy  Want this\n",
      "Similarity:  1.0000000000000002\n",
      "Tweet 1917: Thanks for being top engaged community members this week happy  Want this\n",
      "Tweet 2006: Thanks for being top engaged community members this week happy  Want this ?\n",
      "Similarity:  1.0000000000000002\n",
      "Tweet 1469: miss you unhappy\n",
      "Tweet 1728: I miss you unhappy\n",
      "Similarity:  1.0000000000000002\n",
      "Tweet 524: thanks for the recent follow. Much appreciated happy  Want this\n",
      "Tweet 1674: thanks for the recent follow. Much appreciated happy  Want this?\n",
      "Similarity:  0.9999999999999999\n",
      "Tweet 1141: thanks for the recent follow. Much appreciated happy   Want this ?\n",
      "Tweet 2001: thanks for the recent follow, much appreciated happy   Want this\n",
      "Similarity:  0.9999999999999999\n",
      "Tweet 1744: thanks for the recent follow, much appreciated happy   Want this ?\n",
      "Tweet 1962: thanks for the recent follow. Much appreciated happy  Want this ?\n",
      "Similarity:  0.9999999999999999\n",
      "--------------------------------------------------\n",
      "Preprocessing: lemmatization+misspellings, Vectorization: binary\n",
      "Tweet 77: thank you! happy\n",
      "Tweet 1043: Thank you! happy\n",
      "Similarity:  1.0000000000000002\n",
      "Tweet 423: Thanks for being top engaged community members this week happy   Want this\n",
      "Tweet 1917: Thanks for being top engaged community members this week happy  Want this\n",
      "Similarity:  1.0000000000000002\n",
      "Tweet 1136: When will you notice me? unhappy\n",
      "Tweet 1934: when will you notice me? unhappy\n",
      "Similarity:  1.0000000000000002\n",
      "Tweet 48: Thank you happy\n",
      "Tweet 546: thank you happy\n",
      "Similarity:  1.0000000000000002\n",
      "Tweet 1917: Thanks for being top engaged community members this week happy  Want this\n",
      "Tweet 2006: Thanks for being top engaged community members this week happy  Want this ?\n",
      "Similarity:  1.0000000000000002\n",
      "Tweet 1469: miss you unhappy\n",
      "Tweet 1728: I miss you unhappy\n",
      "Similarity:  1.0000000000000002\n",
      "Tweet 953: Hello everyone, have a great Thursday! Looking forward to reading your tweets happy  Want this\n",
      "Tweet 1492: Hello everyone, have a great Thursday! Looking forward to reading your tweets happy  Want this ?\n",
      "Similarity:  0.9999999999999999\n",
      "Tweet 1141: thanks for the recent follow. Much appreciated happy   Want this ?\n",
      "Tweet 1744: thanks for the recent follow, much appreciated happy   Want this ?\n",
      "Similarity:  0.9999999999999999\n",
      "Tweet 524: thanks for the recent follow. Much appreciated happy  Want this\n",
      "Tweet 1674: thanks for the recent follow. Much appreciated happy  Want this?\n",
      "Similarity:  0.9999999999999999\n",
      "Tweet 1407: thanks for the recent follow. Much appreciated happy   Want this\n",
      "Tweet 1962: thanks for the recent follow. Much appreciated happy  Want this ?\n",
      "Similarity:  0.9999999999999999\n",
      "--------------------------------------------------\n",
      "Preprocessing: just_tokenization, Vectorization: word_count\n",
      "Tweet 48: Thank you happy\n",
      "Tweet 1043: Thank you! happy\n",
      "Similarity:  1.0000000000000002\n",
      "Tweet 366: THANK YOU happy\n",
      "Tweet 546: thank you happy\n",
      "Similarity:  1.0000000000000002\n",
      "Tweet 1136: When will you notice me? unhappy\n",
      "Tweet 1934: when will you notice me? unhappy\n",
      "Similarity:  1.0000000000000002\n",
      "Tweet 1469: miss you unhappy\n",
      "Tweet 1728: I miss you unhappy\n",
      "Similarity:  1.0000000000000002\n",
      "Tweet 566: Thanks for the recent follow Happy to connect happy  have a great Thursday.  Want this\n",
      "Tweet 1663: Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this?\n",
      "Similarity:  1.0\n",
      "Tweet 423: Thanks for being top engaged community members this week happy   Want this\n",
      "Tweet 2006: Thanks for being top engaged community members this week happy  Want this ?\n",
      "Similarity:  1.0\n",
      "Tweet 1415: Thanks for the recent follow Happy to connect happy  have a great Thursday.  Want this ?\n",
      "Tweet 1725: Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this\n",
      "Similarity:  1.0\n",
      "Tweet 1113: Thanks for being top engaged community members this week happy  (Want this ?\n",
      "Tweet 1917: Thanks for being top engaged community members this week happy  Want this\n",
      "Similarity:  1.0\n",
      "Tweet 1429: Thanks for the recent follow Happy to connect happy  have a great Thursday. (Want this?\n",
      "Tweet 1605: Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this ?\n",
      "Similarity:  1.0\n",
      "Tweet 12: Share the love:thanks for being top new followers this week happy   Want this\n",
      "Tweet 869: Share the love: thanks for being top new followers this week happy  Want this?\n",
      "Similarity:  1.0\n",
      "--------------------------------------------------\n",
      "Preprocessing: stemming, Vectorization: word_count\n",
      "Tweet 77: thank you! happy\n",
      "Tweet 546: thank you happy\n",
      "Similarity:  1.0000000000000002\n",
      "Tweet 1469: miss you unhappy\n",
      "Tweet 1728: I miss you unhappy\n",
      "Similarity:  1.0000000000000002\n",
      "Tweet 1136: When will you notice me? unhappy\n",
      "Tweet 1934: when will you notice me? unhappy\n",
      "Similarity:  1.0000000000000002\n",
      "Tweet 546: thank you happy\n",
      "Tweet 1043: Thank you! happy\n",
      "Similarity:  1.0000000000000002\n",
      "Tweet 423: Thanks for being top engaged community members this week happy   Want this\n",
      "Tweet 2006: Thanks for being top engaged community members this week happy  Want this ?\n",
      "Similarity:  1.0\n",
      "Tweet 1605: Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this ?\n",
      "Tweet 1725: Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this\n",
      "Similarity:  1.0\n",
      "Tweet 12: Share the love:thanks for being top new followers this week happy   Want this\n",
      "Tweet 869: Share the love: thanks for being top new followers this week happy  Want this?\n",
      "Similarity:  1.0\n",
      "Tweet 566: Thanks for the recent follow Happy to connect happy  have a great Thursday.  Want this\n",
      "Tweet 1429: Thanks for the recent follow Happy to connect happy  have a great Thursday. (Want this?\n",
      "Similarity:  1.0\n",
      "Tweet 1113: Thanks for being top engaged community members this week happy  (Want this ?\n",
      "Tweet 1917: Thanks for being top engaged community members this week happy  Want this\n",
      "Similarity:  1.0\n",
      "Tweet 1141: thanks for the recent follow. Much appreciated happy   Want this ?\n",
      "Tweet 1962: thanks for the recent follow. Much appreciated happy  Want this ?\n",
      "Similarity:  0.9999999999999999\n",
      "--------------------------------------------------\n",
      "Preprocessing: lemmatization, Vectorization: word_count\n",
      "Tweet 48: Thank you happy\n",
      "Tweet 1043: Thank you! happy\n",
      "Similarity:  1.0000000000000002\n",
      "Tweet 1136: When will you notice me? unhappy\n",
      "Tweet 1934: when will you notice me? unhappy\n",
      "Similarity:  1.0000000000000002\n",
      "Tweet 1469: miss you unhappy\n",
      "Tweet 1728: I miss you unhappy\n",
      "Similarity:  1.0000000000000002\n",
      "Tweet 566: Thanks for the recent follow Happy to connect happy  have a great Thursday.  Want this\n",
      "Tweet 1415: Thanks for the recent follow Happy to connect happy  have a great Thursday.  Want this ?\n",
      "Similarity:  1.0\n",
      "Tweet 1415: Thanks for the recent follow Happy to connect happy  have a great Thursday.  Want this ?\n",
      "Tweet 1725: Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this\n",
      "Similarity:  1.0\n",
      "Tweet 12: Share the love:thanks for being top new followers this week happy   Want this\n",
      "Tweet 869: Share the love: thanks for being top new followers this week happy  Want this?\n",
      "Similarity:  1.0\n",
      "Tweet 1113: Thanks for being top engaged community members this week happy  (Want this ?\n",
      "Tweet 2006: Thanks for being top engaged community members this week happy  Want this ?\n",
      "Similarity:  1.0\n",
      "Tweet 1429: Thanks for the recent follow Happy to connect happy  have a great Thursday. (Want this?\n",
      "Tweet 1605: Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this ?\n",
      "Similarity:  1.0\n",
      "Tweet 1141: thanks for the recent follow. Much appreciated happy   Want this ?\n",
      "Tweet 1407: thanks for the recent follow. Much appreciated happy   Want this\n",
      "Similarity:  0.9999999999999999\n",
      "Tweet 524: thanks for the recent follow. Much appreciated happy  Want this\n",
      "Tweet 1141: thanks for the recent follow. Much appreciated happy   Want this ?\n",
      "Similarity:  0.9999999999999999\n",
      "--------------------------------------------------\n",
      "Preprocessing: stemming+misspellings, Vectorization: word_count\n",
      "Tweet 48: Thank you happy\n",
      "Tweet 546: thank you happy\n",
      "Similarity:  1.0000000000000002\n",
      "Tweet 77: thank you! happy\n",
      "Tweet 1043: Thank you! happy\n",
      "Similarity:  1.0000000000000002\n",
      "Tweet 1469: miss you unhappy\n",
      "Tweet 1728: I miss you unhappy\n",
      "Similarity:  1.0000000000000002\n",
      "Tweet 1136: When will you notice me? unhappy\n",
      "Tweet 1934: when will you notice me? unhappy\n",
      "Similarity:  1.0000000000000002\n",
      "Tweet 566: Thanks for the recent follow Happy to connect happy  have a great Thursday.  Want this\n",
      "Tweet 1429: Thanks for the recent follow Happy to connect happy  have a great Thursday. (Want this?\n",
      "Similarity:  1.0\n",
      "Tweet 1605: Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this ?\n",
      "Tweet 1725: Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this\n",
      "Similarity:  1.0\n",
      "Tweet 423: Thanks for being top engaged community members this week happy   Want this\n",
      "Tweet 2006: Thanks for being top engaged community members this week happy  Want this ?\n",
      "Similarity:  1.0\n",
      "Tweet 12: Share the love:thanks for being top new followers this week happy   Want this\n",
      "Tweet 869: Share the love: thanks for being top new followers this week happy  Want this?\n",
      "Similarity:  1.0\n",
      "Tweet 524: thanks for the recent follow. Much appreciated happy  Want this\n",
      "Tweet 1407: thanks for the recent follow. Much appreciated happy   Want this\n",
      "Similarity:  0.9999999999999999\n",
      "Tweet 1141: thanks for the recent follow. Much appreciated happy   Want this ?\n",
      "Tweet 1744: thanks for the recent follow, much appreciated happy   Want this ?\n",
      "Similarity:  0.9999999999999999\n",
      "--------------------------------------------------\n",
      "Preprocessing: lemmatization+misspellings, Vectorization: word_count\n",
      "Tweet 48: Thank you happy\n",
      "Tweet 77: thank you! happy\n",
      "Similarity:  1.0000000000000002\n",
      "Tweet 366: THANK YOU happy\n",
      "Tweet 1043: Thank you! happy\n",
      "Similarity:  1.0000000000000002\n",
      "Tweet 1136: When will you notice me? unhappy\n",
      "Tweet 1934: when will you notice me? unhappy\n",
      "Similarity:  1.0000000000000002\n",
      "Tweet 77: thank you! happy\n",
      "Tweet 546: thank you happy\n",
      "Similarity:  1.0000000000000002\n",
      "Tweet 1469: miss you unhappy\n",
      "Tweet 1728: I miss you unhappy\n",
      "Similarity:  1.0000000000000002\n",
      "Tweet 566: Thanks for the recent follow Happy to connect happy  have a great Thursday.  Want this\n",
      "Tweet 1725: Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this\n",
      "Similarity:  1.0\n",
      "Tweet 423: Thanks for being top engaged community members this week happy   Want this\n",
      "Tweet 1917: Thanks for being top engaged community members this week happy  Want this\n",
      "Similarity:  1.0\n",
      "Tweet 1113: Thanks for being top engaged community members this week happy  (Want this ?\n",
      "Tweet 2006: Thanks for being top engaged community members this week happy  Want this ?\n",
      "Similarity:  1.0\n",
      "Tweet 1415: Thanks for the recent follow Happy to connect happy  have a great Thursday.  Want this ?\n",
      "Tweet 1429: Thanks for the recent follow Happy to connect happy  have a great Thursday. (Want this?\n",
      "Similarity:  1.0\n",
      "Tweet 1605: Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this ?\n",
      "Tweet 1663: Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this?\n",
      "Similarity:  1.0\n",
      "--------------------------------------------------\n",
      "Preprocessing: just_tokenization, Vectorization: tfidf\n",
      "Tweet 1605: Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this ?\n",
      "Tweet 1663: Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this?\n",
      "Similarity:  1.0000000000000002\n",
      "Tweet 1429: Thanks for the recent follow Happy to connect happy  have a great Thursday. (Want this?\n",
      "Tweet 1725: Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this\n",
      "Similarity:  1.0000000000000002\n",
      "Tweet 77: thank you! happy\n",
      "Tweet 366: THANK YOU happy\n",
      "Similarity:  1.0000000000000002\n",
      "Tweet 566: Thanks for the recent follow Happy to connect happy  have a great Thursday.  Want this\n",
      "Tweet 1605: Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this ?\n",
      "Similarity:  1.0000000000000002\n",
      "Tweet 366: THANK YOU happy\n",
      "Tweet 546: thank you happy\n",
      "Similarity:  1.0000000000000002\n",
      "Tweet 546: thank you happy\n",
      "Tweet 1043: Thank you! happy\n",
      "Similarity:  1.0000000000000002\n",
      "Tweet 1415: Thanks for the recent follow Happy to connect happy  have a great Thursday.  Want this ?\n",
      "Tweet 1429: Thanks for the recent follow Happy to connect happy  have a great Thursday. (Want this?\n",
      "Similarity:  1.0000000000000002\n",
      "Tweet 12: Share the love:thanks for being top new followers this week happy   Want this\n",
      "Tweet 869: Share the love: thanks for being top new followers this week happy  Want this?\n",
      "Similarity:  1.0000000000000002\n",
      "Tweet 428: thanks happy\n",
      "Tweet 1001: .Thanks happy\n",
      "Similarity:  1.0\n",
      "Tweet 524: thanks for the recent follow. Much appreciated happy  Want this\n",
      "Tweet 1141: thanks for the recent follow. Much appreciated happy   Want this ?\n",
      "Similarity:  1.0\n",
      "--------------------------------------------------\n",
      "Preprocessing: stemming, Vectorization: tfidf\n",
      "Tweet 1001: .Thanks happy\n",
      "Tweet 1874: thanks! happy\n",
      "Similarity:  1.0000000000000002\n",
      "Tweet 101: Thanks happy\n",
      "Tweet 428: thanks happy\n",
      "Similarity:  1.0000000000000002\n",
      "Tweet 428: thanks happy\n",
      "Tweet 1001: .Thanks happy\n",
      "Similarity:  1.0000000000000002\n",
      "Tweet 12: Share the love:thanks for being top new followers this week happy   Want this\n",
      "Tweet 869: Share the love: thanks for being top new followers this week happy  Want this?\n",
      "Similarity:  1.0000000000000002\n",
      "Tweet 1407: thanks for the recent follow. Much appreciated happy   Want this\n",
      "Tweet 1744: thanks for the recent follow, much appreciated happy   Want this ?\n",
      "Similarity:  1.0\n",
      "Tweet 423: Thanks for being top engaged community members this week happy   Want this\n",
      "Tweet 1113: Thanks for being top engaged community members this week happy  (Want this ?\n",
      "Similarity:  1.0\n",
      "Tweet 1113: Thanks for being top engaged community members this week happy  (Want this ?\n",
      "Tweet 2006: Thanks for being top engaged community members this week happy  Want this ?\n",
      "Similarity:  1.0\n",
      "Tweet 524: thanks for the recent follow. Much appreciated happy  Want this\n",
      "Tweet 2001: thanks for the recent follow, much appreciated happy   Want this\n",
      "Similarity:  1.0\n",
      "Tweet 1744: thanks for the recent follow, much appreciated happy   Want this ?\n",
      "Tweet 1962: thanks for the recent follow. Much appreciated happy  Want this ?\n",
      "Similarity:  1.0\n",
      "Tweet 467: babies unhappy\n",
      "Tweet 820: baby unhappy\n",
      "Similarity:  1.0\n",
      "--------------------------------------------------\n",
      "Preprocessing: lemmatization, Vectorization: tfidf\n",
      "Tweet 48: Thank you happy\n",
      "Tweet 366: THANK YOU happy\n",
      "Similarity:  1.0000000000000002\n",
      "Tweet 366: THANK YOU happy\n",
      "Tweet 1043: Thank you! happy\n",
      "Similarity:  1.0000000000000002\n",
      "Tweet 77: thank you! happy\n",
      "Tweet 546: thank you happy\n",
      "Similarity:  1.0000000000000002\n",
      "Tweet 12: Share the love:thanks for being top new followers this week happy   Want this\n",
      "Tweet 869: Share the love: thanks for being top new followers this week happy  Want this?\n",
      "Similarity:  1.0000000000000002\n",
      "Tweet 1605: Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this ?\n",
      "Tweet 1725: Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this\n",
      "Similarity:  1.0\n",
      "Tweet 1415: Thanks for the recent follow Happy to connect happy  have a great Thursday.  Want this ?\n",
      "Tweet 1605: Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this ?\n",
      "Similarity:  1.0\n",
      "Tweet 428: thanks happy\n",
      "Tweet 1001: .Thanks happy\n",
      "Similarity:  1.0\n",
      "Tweet 524: thanks for the recent follow. Much appreciated happy  Want this\n",
      "Tweet 1744: thanks for the recent follow, much appreciated happy   Want this ?\n",
      "Similarity:  1.0\n",
      "Tweet 101: Thanks happy\n",
      "Tweet 428: thanks happy\n",
      "Similarity:  1.0\n",
      "Tweet 1141: thanks for the recent follow. Much appreciated happy   Want this ?\n",
      "Tweet 1407: thanks for the recent follow. Much appreciated happy   Want this\n",
      "Similarity:  1.0\n",
      "--------------------------------------------------\n",
      "Preprocessing: stemming+misspellings, Vectorization: tfidf\n",
      "Tweet 524: thanks for the recent follow. Much appreciated happy  Want this\n",
      "Tweet 1674: thanks for the recent follow. Much appreciated happy  Want this?\n",
      "Similarity:  1.0000000000000002\n",
      "Tweet 1141: thanks for the recent follow. Much appreciated happy   Want this ?\n",
      "Tweet 1962: thanks for the recent follow. Much appreciated happy  Want this ?\n",
      "Similarity:  1.0000000000000002\n",
      "Tweet 1407: thanks for the recent follow. Much appreciated happy   Want this\n",
      "Tweet 2001: thanks for the recent follow, much appreciated happy   Want this\n",
      "Similarity:  1.0000000000000002\n",
      "Tweet 1001: .Thanks happy\n",
      "Tweet 1874: thanks! happy\n",
      "Similarity:  1.0000000000000002\n",
      "Tweet 1663: Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this?\n",
      "Tweet 1725: Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this\n",
      "Similarity:  1.0\n",
      "Tweet 566: Thanks for the recent follow Happy to connect happy  have a great Thursday.  Want this\n",
      "Tweet 1605: Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this ?\n",
      "Similarity:  1.0\n",
      "Tweet 632: pake unhappy\n",
      "Tweet 1844: pake 3 unhappy\n",
      "Similarity:  1.0\n",
      "Tweet 1469: miss you unhappy\n",
      "Tweet 1728: I miss you unhappy\n",
      "Similarity:  1.0\n",
      "Tweet 467: babies unhappy\n",
      "Tweet 820: baby unhappy\n",
      "Similarity:  1.0\n",
      "Tweet 366: THANK YOU happy\n",
      "Tweet 1043: Thank you! happy\n",
      "Similarity:  0.9999999999999999\n",
      "--------------------------------------------------\n",
      "Preprocessing: lemmatization+misspellings, Vectorization: tfidf\n",
      "Tweet 1429: Thanks for the recent follow Happy to connect happy  have a great Thursday. (Want this?\n",
      "Tweet 1725: Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this\n",
      "Similarity:  1.0000000000000002\n",
      "Tweet 1605: Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this ?\n",
      "Tweet 1663: Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this?\n",
      "Similarity:  1.0000000000000002\n",
      "Tweet 566: Thanks for the recent follow Happy to connect happy  have a great Thursday.  Want this\n",
      "Tweet 1415: Thanks for the recent follow Happy to connect happy  have a great Thursday.  Want this ?\n",
      "Similarity:  1.0000000000000002\n",
      "Tweet 77: thank you! happy\n",
      "Tweet 546: thank you happy\n",
      "Similarity:  1.0\n",
      "Tweet 48: Thank you happy\n",
      "Tweet 1043: Thank you! happy\n",
      "Similarity:  1.0\n",
      "Tweet 101: Thanks happy\n",
      "Tweet 1001: .Thanks happy\n",
      "Similarity:  1.0\n",
      "Tweet 1469: miss you unhappy\n",
      "Tweet 1728: I miss you unhappy\n",
      "Similarity:  1.0\n",
      "Tweet 974: Hello everyone, have a great Thursday! Looking forward to reading your tweets happy   Want this\n",
      "Tweet 1492: Hello everyone, have a great Thursday! Looking forward to reading your tweets happy  Want this ?\n",
      "Similarity:  1.0\n",
      "Tweet 632: pake unhappy\n",
      "Tweet 1844: pake 3 unhappy\n",
      "Similarity:  1.0\n",
      "Tweet 428: thanks happy\n",
      "Tweet 1874: thanks! happy\n",
      "Similarity:  1.0\n",
      "--------------------------------------------------\n",
      "Preprocessing: just_tokenization, Vectorization: bigrams\n",
      "Tweet 953: Hello everyone, have a great Thursday! Looking forward to reading your tweets happy  Want this\n",
      "Tweet 974: Hello everyone, have a great Thursday! Looking forward to reading your tweets happy   Want this\n",
      "Similarity:  1.0000000000000007\n",
      "Tweet 974: Hello everyone, have a great Thursday! Looking forward to reading your tweets happy   Want this\n",
      "Tweet 1492: Hello everyone, have a great Thursday! Looking forward to reading your tweets happy  Want this ?\n",
      "Similarity:  1.0000000000000007\n",
      "Tweet 1113: Thanks for being top engaged community members this week happy  (Want this ?\n",
      "Tweet 2006: Thanks for being top engaged community members this week happy  Want this ?\n",
      "Similarity:  1.0000000000000004\n",
      "Tweet 423: Thanks for being top engaged community members this week happy   Want this\n",
      "Tweet 1917: Thanks for being top engaged community members this week happy  Want this\n",
      "Similarity:  1.0000000000000004\n",
      "Tweet 632: pake unhappy\n",
      "Tweet 1844: pake 3 unhappy\n",
      "Similarity:  1.0000000000000002\n",
      "Tweet 101: Thanks happy\n",
      "Tweet 428: thanks happy\n",
      "Similarity:  1.0000000000000002\n",
      "Tweet 428: thanks happy\n",
      "Tweet 1001: .Thanks happy\n",
      "Similarity:  1.0000000000000002\n",
      "Tweet 1001: .Thanks happy\n",
      "Tweet 1874: thanks! happy\n",
      "Similarity:  1.0000000000000002\n",
      "Tweet 1136: When will you notice me? unhappy\n",
      "Tweet 1934: when will you notice me? unhappy\n",
      "Similarity:  1.0000000000000002\n",
      "Tweet 546: thank you happy\n",
      "Tweet 1043: Thank you! happy\n",
      "Similarity:  0.9999999999999999\n",
      "--------------------------------------------------\n",
      "Preprocessing: stemming, Vectorization: bigrams\n",
      "Tweet 953: Hello everyone, have a great Thursday! Looking forward to reading your tweets happy  Want this\n",
      "Tweet 974: Hello everyone, have a great Thursday! Looking forward to reading your tweets happy   Want this\n",
      "Similarity:  1.0000000000000007\n",
      "Tweet 974: Hello everyone, have a great Thursday! Looking forward to reading your tweets happy   Want this\n",
      "Tweet 1492: Hello everyone, have a great Thursday! Looking forward to reading your tweets happy  Want this ?\n",
      "Similarity:  1.0000000000000007\n",
      "Tweet 423: Thanks for being top engaged community members this week happy   Want this\n",
      "Tweet 1917: Thanks for being top engaged community members this week happy  Want this\n",
      "Similarity:  1.0000000000000004\n",
      "Tweet 1113: Thanks for being top engaged community members this week happy  (Want this ?\n",
      "Tweet 2006: Thanks for being top engaged community members this week happy  Want this ?\n",
      "Similarity:  1.0000000000000004\n",
      "Tweet 428: thanks happy\n",
      "Tweet 1001: .Thanks happy\n",
      "Similarity:  1.0000000000000002\n",
      "Tweet 101: Thanks happy\n",
      "Tweet 428: thanks happy\n",
      "Similarity:  1.0000000000000002\n",
      "Tweet 632: pake unhappy\n",
      "Tweet 1844: pake 3 unhappy\n",
      "Similarity:  1.0000000000000002\n",
      "Tweet 1136: When will you notice me? unhappy\n",
      "Tweet 1934: when will you notice me? unhappy\n",
      "Similarity:  1.0000000000000002\n",
      "Tweet 1001: .Thanks happy\n",
      "Tweet 1874: thanks! happy\n",
      "Similarity:  1.0000000000000002\n",
      "Tweet 467: babies unhappy\n",
      "Tweet 820: baby unhappy\n",
      "Similarity:  1.0000000000000002\n",
      "--------------------------------------------------\n",
      "Preprocessing: lemmatization, Vectorization: bigrams\n",
      "Tweet 974: Hello everyone, have a great Thursday! Looking forward to reading your tweets happy   Want this\n",
      "Tweet 1492: Hello everyone, have a great Thursday! Looking forward to reading your tweets happy  Want this ?\n",
      "Similarity:  1.0000000000000007\n",
      "Tweet 953: Hello everyone, have a great Thursday! Looking forward to reading your tweets happy  Want this\n",
      "Tweet 974: Hello everyone, have a great Thursday! Looking forward to reading your tweets happy   Want this\n",
      "Similarity:  1.0000000000000007\n",
      "Tweet 1917: Thanks for being top engaged community members this week happy  Want this\n",
      "Tweet 2006: Thanks for being top engaged community members this week happy  Want this ?\n",
      "Similarity:  1.0000000000000004\n",
      "Tweet 423: Thanks for being top engaged community members this week happy   Want this\n",
      "Tweet 1113: Thanks for being top engaged community members this week happy  (Want this ?\n",
      "Similarity:  1.0000000000000004\n",
      "Tweet 101: Thanks happy\n",
      "Tweet 1874: thanks! happy\n",
      "Similarity:  1.0000000000000002\n",
      "Tweet 1136: When will you notice me? unhappy\n",
      "Tweet 1934: when will you notice me? unhappy\n",
      "Similarity:  1.0000000000000002\n",
      "Tweet 428: thanks happy\n",
      "Tweet 1001: .Thanks happy\n",
      "Similarity:  1.0000000000000002\n",
      "Tweet 632: pake unhappy\n",
      "Tweet 1844: pake 3 unhappy\n",
      "Similarity:  1.0000000000000002\n",
      "Tweet 467: babies unhappy\n",
      "Tweet 820: baby unhappy\n",
      "Similarity:  1.0000000000000002\n",
      "Tweet 48: Thank you happy\n",
      "Tweet 546: thank you happy\n",
      "Similarity:  0.9999999999999999\n",
      "--------------------------------------------------\n",
      "Preprocessing: stemming+misspellings, Vectorization: bigrams\n",
      "Tweet 953: Hello everyone, have a great Thursday! Looking forward to reading your tweets happy  Want this\n",
      "Tweet 1492: Hello everyone, have a great Thursday! Looking forward to reading your tweets happy  Want this ?\n",
      "Similarity:  1.0000000000000007\n",
      "Tweet 423: Thanks for being top engaged community members this week happy   Want this\n",
      "Tweet 1113: Thanks for being top engaged community members this week happy  (Want this ?\n",
      "Similarity:  1.0000000000000004\n",
      "Tweet 1113: Thanks for being top engaged community members this week happy  (Want this ?\n",
      "Tweet 1917: Thanks for being top engaged community members this week happy  Want this\n",
      "Similarity:  1.0000000000000004\n",
      "Tweet 1917: Thanks for being top engaged community members this week happy  Want this\n",
      "Tweet 2006: Thanks for being top engaged community members this week happy  Want this ?\n",
      "Similarity:  1.0000000000000004\n",
      "Tweet 1136: When will you notice me? unhappy\n",
      "Tweet 1934: when will you notice me? unhappy\n",
      "Similarity:  1.0000000000000002\n",
      "Tweet 101: Thanks happy\n",
      "Tweet 428: thanks happy\n",
      "Similarity:  1.0000000000000002\n",
      "Tweet 1001: .Thanks happy\n",
      "Tweet 1874: thanks! happy\n",
      "Similarity:  1.0000000000000002\n",
      "Tweet 632: pake unhappy\n",
      "Tweet 1844: pake 3 unhappy\n",
      "Similarity:  1.0000000000000002\n",
      "Tweet 428: thanks happy\n",
      "Tweet 1001: .Thanks happy\n",
      "Similarity:  1.0000000000000002\n",
      "Tweet 467: babies unhappy\n",
      "Tweet 820: baby unhappy\n",
      "Similarity:  1.0000000000000002\n",
      "--------------------------------------------------\n",
      "Preprocessing: lemmatization+misspellings, Vectorization: bigrams\n",
      "Tweet 974: Hello everyone, have a great Thursday! Looking forward to reading your tweets happy   Want this\n",
      "Tweet 1492: Hello everyone, have a great Thursday! Looking forward to reading your tweets happy  Want this ?\n",
      "Similarity:  1.0000000000000007\n",
      "Tweet 953: Hello everyone, have a great Thursday! Looking forward to reading your tweets happy  Want this\n",
      "Tweet 974: Hello everyone, have a great Thursday! Looking forward to reading your tweets happy   Want this\n",
      "Similarity:  1.0000000000000007\n",
      "Tweet 423: Thanks for being top engaged community members this week happy   Want this\n",
      "Tweet 2006: Thanks for being top engaged community members this week happy  Want this ?\n",
      "Similarity:  1.0000000000000004\n",
      "Tweet 1113: Thanks for being top engaged community members this week happy  (Want this ?\n",
      "Tweet 1917: Thanks for being top engaged community members this week happy  Want this\n",
      "Similarity:  1.0000000000000004\n",
      "Tweet 1001: .Thanks happy\n",
      "Tweet 1874: thanks! happy\n",
      "Similarity:  1.0000000000000002\n",
      "Tweet 428: thanks happy\n",
      "Tweet 1001: .Thanks happy\n",
      "Similarity:  1.0000000000000002\n",
      "Tweet 101: Thanks happy\n",
      "Tweet 428: thanks happy\n",
      "Similarity:  1.0000000000000002\n",
      "Tweet 632: pake unhappy\n",
      "Tweet 1844: pake 3 unhappy\n",
      "Similarity:  1.0000000000000002\n",
      "Tweet 467: babies unhappy\n",
      "Tweet 820: baby unhappy\n",
      "Similarity:  1.0000000000000002\n",
      "Tweet 1136: When will you notice me? unhappy\n",
      "Tweet 1934: when will you notice me? unhappy\n",
      "Similarity:  1.0000000000000002\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Calculate the top 10 most similar tweets for each vectorization and preprocessing technique\n",
    "for v in vectorization_techniques:\n",
    "    for p in preprocessing_techniques:\n",
    "        print(f'Preprocessing: {p}, Vectorization: {v}')\n",
    "        vectorizer = results.loc[p, v]\n",
    "        X_train_vectorized = vectorizer.transform(X_train)\n",
    "        top = find_similar_tweets(X_train_vectorized, normalized=(v == 'tfidf'))\n",
    "        for i in top.index:\n",
    "            idx_1 = top['tweet1'][i]\n",
    "            idx_2 = top['tweet2'][i]\n",
    "            print(f'Tweet {idx_1}:', X_train.iloc[idx_1])\n",
    "            print(f'Tweet {idx_2}:', X_train.iloc[idx_2])\n",
    "            print('Similarity: ', top['similarity'][i])\n",
    "        print('-' * 50)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Machine learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try different algorithms and different datasets that you prepared before to solve the classification\n",
    "task – sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = [KNeighborsClassifier(), LogisticRegression(), RandomForestClassifier(), SVC(), ComplementNB()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'KNeighborsClassifier': KNeighborsClassifier(),\n",
       " 'LogisticRegression': LogisticRegression(),\n",
       " 'RandomForestClassifier': RandomForestClassifier(),\n",
       " 'SVC': SVC(),\n",
       " 'ComplementNB': ComplementNB()}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert classifiers to dictionary\n",
    "classifiers_dict = {type(c).__name__: c for c in classifiers}\n",
    "classifiers_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe to store results\n",
    "accuracy = pd.DataFrame(columns=vectorization_techniques, index=preprocessing_techniques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier\n",
      "                              binary word_count     tfidf   bigrams\n",
      "just_tokenization           0.609982   0.621072  0.759704  0.534196\n",
      "stemming                    0.600739   0.617375  0.759704  0.530499\n",
      "lemmatization               0.608133   0.622921   0.76525  0.537893\n",
      "stemming+misspellings       0.617375   0.622921  0.746765  0.532348\n",
      "lemmatization+misspellings   0.61183   0.617375  0.748614  0.537893\n",
      "\n",
      "LogisticRegression\n",
      "                              binary word_count     tfidf   bigrams\n",
      "just_tokenization           0.922366   0.913124  0.913124  0.911275\n",
      "stemming                    0.916821   0.911275  0.909427  0.909427\n",
      "lemmatization               0.911275   0.902033  0.913124   0.90573\n",
      "stemming+misspellings       0.916821   0.909427  0.913124  0.909427\n",
      "lemmatization+misspellings  0.909427   0.902033  0.913124   0.90573\n",
      "\n",
      "RandomForestClassifier\n",
      "                              binary word_count     tfidf   bigrams\n",
      "just_tokenization           0.909427   0.922366  0.926063   0.89464\n",
      "stemming                    0.927911   0.920518  0.926063  0.902033\n",
      "lemmatization               0.914972   0.911275  0.918669  0.892791\n",
      "stemming+misspellings       0.914972    0.90573  0.909427  0.892791\n",
      "lemmatization+misspellings  0.916821   0.913124   0.90573  0.902033\n",
      "\n",
      "SVC\n",
      "                              binary word_count     tfidf   bigrams\n",
      "just_tokenization           0.907579    0.90573  0.924214  0.896488\n",
      "stemming                    0.911275   0.903882  0.924214  0.898336\n",
      "lemmatization               0.909427    0.90573  0.924214  0.896488\n",
      "stemming+misspellings       0.913124    0.90573  0.924214  0.898336\n",
      "lemmatization+misspellings  0.907579    0.90573  0.924214  0.896488\n",
      "\n",
      "ComplementNB\n",
      "                              binary word_count     tfidf   bigrams\n",
      "just_tokenization           0.931608   0.931608  0.922366  0.924214\n",
      "stemming                    0.935305   0.937153  0.913124  0.927911\n",
      "lemmatization                0.92976   0.933457  0.914972  0.924214\n",
      "stemming+misspellings       0.927911   0.931608  0.909427  0.913124\n",
      "lemmatization+misspellings  0.926063   0.933457  0.909427  0.918669\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for classifier in classifiers_dict:\n",
    "    print(classifier)\n",
    "    clf = classifiers_dict[classifier]\n",
    "    for v in vectorization_techniques:\n",
    "        for p in preprocessing_techniques:\n",
    "            vectorizer = results.loc[p, v]\n",
    "            X_train_vectorized = vectorizer.transform(X_train)\n",
    "            X_test_vectorized = vectorizer.transform(X_test)\n",
    "            clf.fit(X_train_vectorized, y_train)\n",
    "            y_pred = clf.predict(X_test_vectorized)\n",
    "            acc_score = accuracy_score(y_test, y_pred)\n",
    "            accuracy.loc[p, v] = acc_score\n",
    "    print(accuracy)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.89      0.95      0.92       173\n",
      "           0       0.97      0.96      0.96       202\n",
      "           1       0.95      0.89      0.92       166\n",
      "\n",
      "    accuracy                           0.94       541\n",
      "   macro avg       0.94      0.94      0.94       541\n",
      "weighted avg       0.94      0.94      0.94       541\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectorizer = results.loc['stemming', 'word_count']\n",
    "X_train_vectorized = vectorizer.transform(X_train)\n",
    "X_test_vectorized = vectorizer.transform(X_test)\n",
    "\n",
    "best_classifier = classifiers_dict['ComplementNB']\n",
    "best_classifier.fit(X_train_vectorized, y_train)\n",
    "y_pred = best_classifier.predict(X_test_vectorized)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "42AI-mfarhi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
