{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Import useful modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/mfarhi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/mfarhi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package words to /Users/mfarhi/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import words\n",
    "from nltk.metrics.distance import edit_distance\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity, linear_kernel\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('words')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Load and prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/processedPositive.txt') as f:\n",
    "    pos_data = f.read().splitlines()\n",
    "with open('data/processedNegative.txt') as f:\n",
    "    neg_data = f.read().splitlines()\n",
    "with open('data/processedNeutral.txt') as f:\n",
    "    neu_data = f.read().splitlines()\n",
    "\n",
    "processed_positive_df = pd.DataFrame({'tweets': pos_data, 'labels': 1})\n",
    "processed_negative_df = pd.DataFrame({'tweets': neg_data, 'labels': -1})\n",
    "processed_neutral_df = pd.DataFrame({'tweets': neu_data, 'labels': 0})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenate all three categories into one dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([processed_positive_df, processed_negative_df, processed_neutral_df], ignore_index=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.duplicated() == False]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset into 80% training and 20% test with stratification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['tweets'], df['labels'], test_size=0.2, stratify = df['labels'], random_state=1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure the split was stratified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0    0.373797\n",
       "-1    0.318653\n",
       " 1    0.307550\n",
       "Name: labels, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['labels'].value_counts() / len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0    0.373901\n",
       "-1    0.318371\n",
       " 1    0.307728\n",
       "Name: labels, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts() / len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0    0.373383\n",
       "-1    0.319778\n",
       " 1    0.306839\n",
       "Name: labels, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts() / len(y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try different vectorization techniques: binary, word count, TFIDF and bigrams. We will also try different preprocessing techniques: simple tokenization, stemming, lemmatization, stemming + misspellings and lemmatization + misspellings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter_stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StemVectorizer(CountVectorizer):\n",
    "    def build_tokenizer(self):\n",
    "        tokenizer = super().build_tokenizer()\n",
    "        return lambda doc: list(map(porter_stemmer.stem, tokenizer(doc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StemTfidfVectorizer(TfidfVectorizer):\n",
    "    def build_tokenizer(self):\n",
    "        tokenizer = super().build_tokenizer()\n",
    "        return lambda doc: list(map(porter_stemmer.stem, tokenizer(doc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LemmaVectorizer(CountVectorizer):\n",
    "    def build_tokenizer(self):\n",
    "        tokenizer = super().build_tokenizer()\n",
    "        return lambda doc: list(map(lemmatizer.lemmatize, tokenizer(doc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LemmaTfidfVectorizer(TfidfVectorizer):\n",
    "    def build_tokenizer(self):\n",
    "        tokenizer = super().build_tokenizer()\n",
    "        return lambda doc: list(map(lemmatizer.lemmatize, tokenizer(doc)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the vocabulary we will be using to correct misspellings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {word.lower() for word in words.words()}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use caching (memoization) to speed up the correction of misspelled words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def memoize(f):\n",
    "    cache = {}\n",
    "    def memoized_f(*args):\n",
    "        if args not in cache:\n",
    "            cache[args] = f(*args)\n",
    "        return cache[args]\n",
    "    return memoized_f\n",
    "\n",
    "@memoize\n",
    "def correct_word(word):\n",
    "    distances = [(w, edit_distance(word, w)) for w in vocab if (w[0] == word[0] and w[-1] == word[-1] and len(w) - len(word) in {-1, 0, 1})]\n",
    "    return min(distances, key=lambda x: x[1])[0] if distances else word\n",
    "\n",
    "def autocorrect(tokens):\n",
    "    return [word if (word in vocab or (not word.isalpha())) else correct_word(word) for word in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StemAutocorrectVectorizer(CountVectorizer):\n",
    "    def build_tokenizer(self):\n",
    "        tokenizer = super().build_tokenizer()\n",
    "        return lambda doc: autocorrect(list(map(porter_stemmer.stem, tokenizer(doc))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StemAutocorrectTfidfVectorizer(TfidfVectorizer):\n",
    "    def build_tokenizer(self):\n",
    "        tokenizer = super().build_tokenizer()\n",
    "        return lambda doc: autocorrect(list(map(porter_stemmer.stem, tokenizer(doc))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LemmaAutocorrectVectorizer(CountVectorizer):\n",
    "    def build_tokenizer(self):\n",
    "        tokenizer = super().build_tokenizer()\n",
    "        return lambda doc: autocorrect(list(map(lemmatizer.lemmatize, tokenizer(doc))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LemmaAutocorrectTfidfVectorizer(TfidfVectorizer):\n",
    "    def build_tokenizer(self):\n",
    "        tokenizer = super().build_tokenizer()\n",
    "        return lambda doc: autocorrect(list(map(lemmatizer.lemmatize, tokenizer(doc))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorization_techniques = ['binary', 'word_count', 'tfidf', 'bigrams']\n",
    "preprocessing_techniques = ['just_tokenization', 'stemming', 'lemmatization', 'stemming+misspellings', 'lemmatization+misspellings']\n",
    "## create dataframe to store results\n",
    "results = pd.DataFrame(columns=vectorization_techniques, index=preprocessing_techniques)\n",
    "for v in vectorization_techniques:\n",
    "    for p in preprocessing_techniques:\n",
    "        if v == 'word_count' or v == 'binary':\n",
    "            if p == 'just_tokenization':\n",
    "                vectorizer = CountVectorizer(binary=(v == 'binary'))\n",
    "            elif p == 'stemming':\n",
    "                vectorizer = StemVectorizer(binary=(v == 'binary'))\n",
    "            elif p == 'lemmatization':\n",
    "                vectorizer = LemmaVectorizer(binary=(v == 'binary'))\n",
    "            elif p == 'stemming+misspellings':\n",
    "                vectorizer = StemAutocorrectVectorizer(binary=(v == 'binary'))\n",
    "            elif p == 'lemmatization+misspellings':\n",
    "                vectorizer = LemmaAutocorrectVectorizer(binary=(v == 'binary'))\n",
    "        elif v == 'tfidf':\n",
    "            if p == 'just_tokenization':\n",
    "                vectorizer = TfidfVectorizer()\n",
    "            elif p == 'stemming':\n",
    "                vectorizer = StemTfidfVectorizer()\n",
    "            elif p == 'lemmatization':\n",
    "                vectorizer = LemmaTfidfVectorizer()\n",
    "            elif p == 'stemming+misspellings':\n",
    "                vectorizer = StemAutocorrectTfidfVectorizer()\n",
    "            elif p == 'lemmatization+misspellings':\n",
    "                vectorizer = LemmaAutocorrectTfidfVectorizer()\n",
    "        elif v == 'bigrams':\n",
    "            if p == 'just_tokenization':\n",
    "                vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "            elif p == 'stemming':\n",
    "                vectorizer = StemVectorizer(ngram_range=(1, 2))\n",
    "            elif p == 'lemmatization':\n",
    "                vectorizer = LemmaVectorizer(ngram_range=(1, 2))\n",
    "            elif p == 'stemming+misspellings':\n",
    "                vectorizer = StemAutocorrectVectorizer(ngram_range=(1, 2))\n",
    "            elif p == 'lemmatization+misspellings':\n",
    "                vectorizer = LemmaAutocorrectVectorizer(ngram_range=(1, 2))\n",
    "        vectorizer.fit(X_train)\n",
    "        results.loc[p, v] = vectorizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Similarity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the different approaches that we prepared in the task above and cosine similarity to find the top-10 most similar pairs of tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_tweets(X_vec, normalized=False, n=10):\n",
    "    similarity = linear_kernel(X_vec, X_vec) if normalized else cosine_similarity(X_vec, X_vec)\n",
    "    similarity = pd.DataFrame(similarity)\n",
    "    similarity = similarity.stack().reset_index()\n",
    "    similarity.columns = ['tweet1', 'tweet2', 'similarity']\n",
    "    similarity = similarity[similarity['tweet1'] < similarity['tweet2']]\n",
    "    similarity = similarity.sort_values(by='similarity', ascending=False)\n",
    "    similarity = similarity[similarity.duplicated(subset='tweet1') == False]\n",
    "    similarity = similarity[similarity.duplicated(subset='tweet2') == False]\n",
    "    top = similarity.head(n)\n",
    "    return top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 most similar tweets using just_tokenization for preprocessing and binary for vectorization:\n",
      "Tweet 77: thank you! happy\n",
      "Tweet 546: thank you happy\n",
      "Similarity: 1.00\n",
      "Tweet 1917: Thanks for being top engaged community members this week happy  Want this\n",
      "Tweet 2006: Thanks for being top engaged community members this week happy  Want this ?\n",
      "Similarity: 1.00\n",
      "Tweet 1136: When will you notice me? unhappy\n",
      "Tweet 1934: when will you notice me? unhappy\n",
      "Similarity: 1.00\n",
      "Tweet 546: thank you happy\n",
      "Tweet 1043: Thank you! happy\n",
      "Similarity: 1.00\n",
      "Tweet 48: Thank you happy\n",
      "Tweet 77: thank you! happy\n",
      "Similarity: 1.00\n",
      "Tweet 1469: miss you unhappy\n",
      "Tweet 1728: I miss you unhappy\n",
      "Similarity: 1.00\n",
      "Tweet 1141: thanks for the recent follow. Much appreciated happy   Want this ?\n",
      "Tweet 1744: thanks for the recent follow, much appreciated happy   Want this ?\n",
      "Similarity: 1.00\n",
      "Tweet 953: Hello everyone, have a great Thursday! Looking forward to reading your tweets happy  Want this\n",
      "Tweet 974: Hello everyone, have a great Thursday! Looking forward to reading your tweets happy   Want this\n",
      "Similarity: 1.00\n",
      "Tweet 1407: thanks for the recent follow. Much appreciated happy   Want this\n",
      "Tweet 1962: thanks for the recent follow. Much appreciated happy  Want this ?\n",
      "Similarity: 1.00\n",
      "Tweet 524: thanks for the recent follow. Much appreciated happy  Want this\n",
      "Tweet 1141: thanks for the recent follow. Much appreciated happy   Want this ?\n",
      "Similarity: 1.00\n",
      "--------------------------------------------------------------------------------\n",
      "Top 10 most similar tweets using stemming for preprocessing and binary for vectorization:\n",
      "Tweet 77: thank you! happy\n",
      "Tweet 1043: Thank you! happy\n",
      "Similarity: 1.00\n",
      "Tweet 1113: Thanks for being top engaged community members this week happy  (Want this ?\n",
      "Tweet 1917: Thanks for being top engaged community members this week happy  Want this\n",
      "Similarity: 1.00\n",
      "Tweet 1469: miss you unhappy\n",
      "Tweet 1728: I miss you unhappy\n",
      "Similarity: 1.00\n",
      "Tweet 1917: Thanks for being top engaged community members this week happy  Want this\n",
      "Tweet 2006: Thanks for being top engaged community members this week happy  Want this ?\n",
      "Similarity: 1.00\n",
      "Tweet 1136: When will you notice me? unhappy\n",
      "Tweet 1934: when will you notice me? unhappy\n",
      "Similarity: 1.00\n",
      "Tweet 1407: thanks for the recent follow. Much appreciated happy   Want this\n",
      "Tweet 1674: thanks for the recent follow. Much appreciated happy  Want this?\n",
      "Similarity: 1.00\n",
      "Tweet 524: thanks for the recent follow. Much appreciated happy  Want this\n",
      "Tweet 2001: thanks for the recent follow, much appreciated happy   Want this\n",
      "Similarity: 1.00\n",
      "Tweet 1674: thanks for the recent follow. Much appreciated happy  Want this?\n",
      "Tweet 1744: thanks for the recent follow, much appreciated happy   Want this ?\n",
      "Similarity: 1.00\n",
      "Tweet 974: Hello everyone, have a great Thursday! Looking forward to reading your tweets happy   Want this\n",
      "Tweet 1492: Hello everyone, have a great Thursday! Looking forward to reading your tweets happy  Want this ?\n",
      "Similarity: 1.00\n",
      "Tweet 1429: Thanks for the recent follow Happy to connect happy  have a great Thursday. (Want this?\n",
      "Tweet 1663: Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this?\n",
      "Similarity: 1.00\n",
      "--------------------------------------------------------------------------------\n",
      "Top 10 most similar tweets using lemmatization for preprocessing and binary for vectorization:\n",
      "Tweet 77: thank you! happy\n",
      "Tweet 366: THANK YOU happy\n",
      "Similarity: 1.00\n",
      "Tweet 546: thank you happy\n",
      "Tweet 1043: Thank you! happy\n",
      "Similarity: 1.00\n",
      "Tweet 366: THANK YOU happy\n",
      "Tweet 546: thank you happy\n",
      "Similarity: 1.00\n",
      "Tweet 1917: Thanks for being top engaged community members this week happy  Want this\n",
      "Tweet 2006: Thanks for being top engaged community members this week happy  Want this ?\n",
      "Similarity: 1.00\n",
      "Tweet 1469: miss you unhappy\n",
      "Tweet 1728: I miss you unhappy\n",
      "Similarity: 1.00\n",
      "Tweet 423: Thanks for being top engaged community members this week happy   Want this\n",
      "Tweet 1113: Thanks for being top engaged community members this week happy  (Want this ?\n",
      "Similarity: 1.00\n",
      "Tweet 1136: When will you notice me? unhappy\n",
      "Tweet 1934: when will you notice me? unhappy\n",
      "Similarity: 1.00\n",
      "Tweet 1141: thanks for the recent follow. Much appreciated happy   Want this ?\n",
      "Tweet 1962: thanks for the recent follow. Much appreciated happy  Want this ?\n",
      "Similarity: 1.00\n",
      "Tweet 953: Hello everyone, have a great Thursday! Looking forward to reading your tweets happy  Want this\n",
      "Tweet 974: Hello everyone, have a great Thursday! Looking forward to reading your tweets happy   Want this\n",
      "Similarity: 1.00\n",
      "Tweet 1962: thanks for the recent follow. Much appreciated happy  Want this ?\n",
      "Tweet 2001: thanks for the recent follow, much appreciated happy   Want this\n",
      "Similarity: 1.00\n",
      "--------------------------------------------------------------------------------\n",
      "Top 10 most similar tweets using stemming+misspellings for preprocessing and binary for vectorization:\n",
      "Tweet 366: THANK YOU happy\n",
      "Tweet 1043: Thank you! happy\n",
      "Similarity: 1.00\n",
      "Tweet 48: Thank you happy\n",
      "Tweet 366: THANK YOU happy\n",
      "Similarity: 1.00\n",
      "Tweet 423: Thanks for being top engaged community members this week happy   Want this\n",
      "Tweet 1917: Thanks for being top engaged community members this week happy  Want this\n",
      "Similarity: 1.00\n",
      "Tweet 1917: Thanks for being top engaged community members this week happy  Want this\n",
      "Tweet 2006: Thanks for being top engaged community members this week happy  Want this ?\n",
      "Similarity: 1.00\n",
      "Tweet 1469: miss you unhappy\n",
      "Tweet 1728: I miss you unhappy\n",
      "Similarity: 1.00\n",
      "Tweet 1136: When will you notice me? unhappy\n",
      "Tweet 1934: when will you notice me? unhappy\n",
      "Similarity: 1.00\n",
      "Tweet 1962: thanks for the recent follow. Much appreciated happy  Want this ?\n",
      "Tweet 2001: thanks for the recent follow, much appreciated happy   Want this\n",
      "Similarity: 1.00\n",
      "Tweet 1141: thanks for the recent follow. Much appreciated happy   Want this ?\n",
      "Tweet 1674: thanks for the recent follow. Much appreciated happy  Want this?\n",
      "Similarity: 1.00\n",
      "Tweet 524: thanks for the recent follow. Much appreciated happy  Want this\n",
      "Tweet 1141: thanks for the recent follow. Much appreciated happy   Want this ?\n",
      "Similarity: 1.00\n",
      "Tweet 953: Hello everyone, have a great Thursday! Looking forward to reading your tweets happy  Want this\n",
      "Tweet 974: Hello everyone, have a great Thursday! Looking forward to reading your tweets happy   Want this\n",
      "Similarity: 1.00\n",
      "--------------------------------------------------------------------------------\n",
      "Top 10 most similar tweets using lemmatization+misspellings for preprocessing and binary for vectorization:\n",
      "Tweet 77: thank you! happy\n",
      "Tweet 366: THANK YOU happy\n",
      "Similarity: 1.00\n",
      "Tweet 1136: When will you notice me? unhappy\n",
      "Tweet 1934: when will you notice me? unhappy\n",
      "Similarity: 1.00\n",
      "Tweet 423: Thanks for being top engaged community members this week happy   Want this\n",
      "Tweet 1113: Thanks for being top engaged community members this week happy  (Want this ?\n",
      "Similarity: 1.00\n",
      "Tweet 1469: miss you unhappy\n",
      "Tweet 1728: I miss you unhappy\n",
      "Similarity: 1.00\n",
      "Tweet 546: thank you happy\n",
      "Tweet 1043: Thank you! happy\n",
      "Similarity: 1.00\n",
      "Tweet 1113: Thanks for being top engaged community members this week happy  (Want this ?\n",
      "Tweet 2006: Thanks for being top engaged community members this week happy  Want this ?\n",
      "Similarity: 1.00\n",
      "Tweet 366: THANK YOU happy\n",
      "Tweet 546: thank you happy\n",
      "Similarity: 1.00\n",
      "Tweet 1674: thanks for the recent follow. Much appreciated happy  Want this?\n",
      "Tweet 1962: thanks for the recent follow. Much appreciated happy  Want this ?\n",
      "Similarity: 1.00\n",
      "Tweet 524: thanks for the recent follow. Much appreciated happy  Want this\n",
      "Tweet 1141: thanks for the recent follow. Much appreciated happy   Want this ?\n",
      "Similarity: 1.00\n",
      "Tweet 1141: thanks for the recent follow. Much appreciated happy   Want this ?\n",
      "Tweet 1744: thanks for the recent follow, much appreciated happy   Want this ?\n",
      "Similarity: 1.00\n",
      "--------------------------------------------------------------------------------\n",
      "Top 10 most similar tweets using just_tokenization for preprocessing and word_count for vectorization:\n",
      "Tweet 48: Thank you happy\n",
      "Tweet 1043: Thank you! happy\n",
      "Similarity: 1.00\n",
      "Tweet 366: THANK YOU happy\n",
      "Tweet 546: thank you happy\n",
      "Similarity: 1.00\n",
      "Tweet 1136: When will you notice me? unhappy\n",
      "Tweet 1934: when will you notice me? unhappy\n",
      "Similarity: 1.00\n",
      "Tweet 1469: miss you unhappy\n",
      "Tweet 1728: I miss you unhappy\n",
      "Similarity: 1.00\n",
      "Tweet 566: Thanks for the recent follow Happy to connect happy  have a great Thursday.  Want this\n",
      "Tweet 1663: Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this?\n",
      "Similarity: 1.00\n",
      "Tweet 423: Thanks for being top engaged community members this week happy   Want this\n",
      "Tweet 2006: Thanks for being top engaged community members this week happy  Want this ?\n",
      "Similarity: 1.00\n",
      "Tweet 1415: Thanks for the recent follow Happy to connect happy  have a great Thursday.  Want this ?\n",
      "Tweet 1725: Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this\n",
      "Similarity: 1.00\n",
      "Tweet 1113: Thanks for being top engaged community members this week happy  (Want this ?\n",
      "Tweet 1917: Thanks for being top engaged community members this week happy  Want this\n",
      "Similarity: 1.00\n",
      "Tweet 1429: Thanks for the recent follow Happy to connect happy  have a great Thursday. (Want this?\n",
      "Tweet 1605: Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this ?\n",
      "Similarity: 1.00\n",
      "Tweet 12: Share the love:thanks for being top new followers this week happy   Want this\n",
      "Tweet 869: Share the love: thanks for being top new followers this week happy  Want this?\n",
      "Similarity: 1.00\n",
      "--------------------------------------------------------------------------------\n",
      "Top 10 most similar tweets using stemming for preprocessing and word_count for vectorization:\n",
      "Tweet 77: thank you! happy\n",
      "Tweet 546: thank you happy\n",
      "Similarity: 1.00\n",
      "Tweet 1469: miss you unhappy\n",
      "Tweet 1728: I miss you unhappy\n",
      "Similarity: 1.00\n",
      "Tweet 1136: When will you notice me? unhappy\n",
      "Tweet 1934: when will you notice me? unhappy\n",
      "Similarity: 1.00\n",
      "Tweet 546: thank you happy\n",
      "Tweet 1043: Thank you! happy\n",
      "Similarity: 1.00\n",
      "Tweet 423: Thanks for being top engaged community members this week happy   Want this\n",
      "Tweet 2006: Thanks for being top engaged community members this week happy  Want this ?\n",
      "Similarity: 1.00\n",
      "Tweet 1605: Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this ?\n",
      "Tweet 1725: Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this\n",
      "Similarity: 1.00\n",
      "Tweet 12: Share the love:thanks for being top new followers this week happy   Want this\n",
      "Tweet 869: Share the love: thanks for being top new followers this week happy  Want this?\n",
      "Similarity: 1.00\n",
      "Tweet 566: Thanks for the recent follow Happy to connect happy  have a great Thursday.  Want this\n",
      "Tweet 1429: Thanks for the recent follow Happy to connect happy  have a great Thursday. (Want this?\n",
      "Similarity: 1.00\n",
      "Tweet 1113: Thanks for being top engaged community members this week happy  (Want this ?\n",
      "Tweet 1917: Thanks for being top engaged community members this week happy  Want this\n",
      "Similarity: 1.00\n",
      "Tweet 1141: thanks for the recent follow. Much appreciated happy   Want this ?\n",
      "Tweet 1962: thanks for the recent follow. Much appreciated happy  Want this ?\n",
      "Similarity: 1.00\n",
      "--------------------------------------------------------------------------------\n",
      "Top 10 most similar tweets using lemmatization for preprocessing and word_count for vectorization:\n",
      "Tweet 48: Thank you happy\n",
      "Tweet 1043: Thank you! happy\n",
      "Similarity: 1.00\n",
      "Tweet 1136: When will you notice me? unhappy\n",
      "Tweet 1934: when will you notice me? unhappy\n",
      "Similarity: 1.00\n",
      "Tweet 1469: miss you unhappy\n",
      "Tweet 1728: I miss you unhappy\n",
      "Similarity: 1.00\n",
      "Tweet 566: Thanks for the recent follow Happy to connect happy  have a great Thursday.  Want this\n",
      "Tweet 1415: Thanks for the recent follow Happy to connect happy  have a great Thursday.  Want this ?\n",
      "Similarity: 1.00\n",
      "Tweet 1415: Thanks for the recent follow Happy to connect happy  have a great Thursday.  Want this ?\n",
      "Tweet 1725: Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this\n",
      "Similarity: 1.00\n",
      "Tweet 12: Share the love:thanks for being top new followers this week happy   Want this\n",
      "Tweet 869: Share the love: thanks for being top new followers this week happy  Want this?\n",
      "Similarity: 1.00\n",
      "Tweet 1113: Thanks for being top engaged community members this week happy  (Want this ?\n",
      "Tweet 2006: Thanks for being top engaged community members this week happy  Want this ?\n",
      "Similarity: 1.00\n",
      "Tweet 1429: Thanks for the recent follow Happy to connect happy  have a great Thursday. (Want this?\n",
      "Tweet 1605: Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this ?\n",
      "Similarity: 1.00\n",
      "Tweet 1141: thanks for the recent follow. Much appreciated happy   Want this ?\n",
      "Tweet 1407: thanks for the recent follow. Much appreciated happy   Want this\n",
      "Similarity: 1.00\n",
      "Tweet 524: thanks for the recent follow. Much appreciated happy  Want this\n",
      "Tweet 1141: thanks for the recent follow. Much appreciated happy   Want this ?\n",
      "Similarity: 1.00\n",
      "--------------------------------------------------------------------------------\n",
      "Top 10 most similar tweets using stemming+misspellings for preprocessing and word_count for vectorization:\n",
      "Tweet 48: Thank you happy\n",
      "Tweet 77: thank you! happy\n",
      "Similarity: 1.00\n",
      "Tweet 77: thank you! happy\n",
      "Tweet 546: thank you happy\n",
      "Similarity: 1.00\n",
      "Tweet 1469: miss you unhappy\n",
      "Tweet 1728: I miss you unhappy\n",
      "Similarity: 1.00\n",
      "Tweet 366: THANK YOU happy\n",
      "Tweet 1043: Thank you! happy\n",
      "Similarity: 1.00\n",
      "Tweet 1136: When will you notice me? unhappy\n",
      "Tweet 1934: when will you notice me? unhappy\n",
      "Similarity: 1.00\n",
      "Tweet 1663: Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this?\n",
      "Tweet 1725: Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this\n",
      "Similarity: 1.00\n",
      "Tweet 1429: Thanks for the recent follow Happy to connect happy  have a great Thursday. (Want this?\n",
      "Tweet 1605: Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this ?\n",
      "Similarity: 1.00\n",
      "Tweet 423: Thanks for being top engaged community members this week happy   Want this\n",
      "Tweet 1917: Thanks for being top engaged community members this week happy  Want this\n",
      "Similarity: 1.00\n",
      "Tweet 1605: Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this ?\n",
      "Tweet 1663: Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this?\n",
      "Similarity: 1.00\n",
      "Tweet 1917: Thanks for being top engaged community members this week happy  Want this\n",
      "Tweet 2006: Thanks for being top engaged community members this week happy  Want this ?\n",
      "Similarity: 1.00\n",
      "--------------------------------------------------------------------------------\n",
      "Top 10 most similar tweets using lemmatization+misspellings for preprocessing and word_count for vectorization:\n",
      "Tweet 1136: When will you notice me? unhappy\n",
      "Tweet 1934: when will you notice me? unhappy\n",
      "Similarity: 1.00\n",
      "Tweet 1469: miss you unhappy\n",
      "Tweet 1728: I miss you unhappy\n",
      "Similarity: 1.00\n",
      "Tweet 366: THANK YOU happy\n",
      "Tweet 546: thank you happy\n",
      "Similarity: 1.00\n",
      "Tweet 546: thank you happy\n",
      "Tweet 1043: Thank you! happy\n",
      "Similarity: 1.00\n",
      "Tweet 48: Thank you happy\n",
      "Tweet 77: thank you! happy\n",
      "Similarity: 1.00\n",
      "Tweet 1429: Thanks for the recent follow Happy to connect happy  have a great Thursday. (Want this?\n",
      "Tweet 1663: Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this?\n",
      "Similarity: 1.00\n",
      "Tweet 1663: Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this?\n",
      "Tweet 1725: Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this\n",
      "Similarity: 1.00\n",
      "Tweet 1113: Thanks for being top engaged community members this week happy  (Want this ?\n",
      "Tweet 1917: Thanks for being top engaged community members this week happy  Want this\n",
      "Similarity: 1.00\n",
      "Tweet 12: Share the love:thanks for being top new followers this week happy   Want this\n",
      "Tweet 869: Share the love: thanks for being top new followers this week happy  Want this?\n",
      "Similarity: 1.00\n",
      "Tweet 1917: Thanks for being top engaged community members this week happy  Want this\n",
      "Tweet 2006: Thanks for being top engaged community members this week happy  Want this ?\n",
      "Similarity: 1.00\n",
      "--------------------------------------------------------------------------------\n",
      "Top 10 most similar tweets using just_tokenization for preprocessing and tfidf for vectorization:\n",
      "Tweet 1605: Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this ?\n",
      "Tweet 1663: Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this?\n",
      "Similarity: 1.00\n",
      "Tweet 1429: Thanks for the recent follow Happy to connect happy  have a great Thursday. (Want this?\n",
      "Tweet 1725: Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this\n",
      "Similarity: 1.00\n",
      "Tweet 77: thank you! happy\n",
      "Tweet 366: THANK YOU happy\n",
      "Similarity: 1.00\n",
      "Tweet 566: Thanks for the recent follow Happy to connect happy  have a great Thursday.  Want this\n",
      "Tweet 1605: Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this ?\n",
      "Similarity: 1.00\n",
      "Tweet 366: THANK YOU happy\n",
      "Tweet 546: thank you happy\n",
      "Similarity: 1.00\n",
      "Tweet 546: thank you happy\n",
      "Tweet 1043: Thank you! happy\n",
      "Similarity: 1.00\n",
      "Tweet 1415: Thanks for the recent follow Happy to connect happy  have a great Thursday.  Want this ?\n",
      "Tweet 1429: Thanks for the recent follow Happy to connect happy  have a great Thursday. (Want this?\n",
      "Similarity: 1.00\n",
      "Tweet 12: Share the love:thanks for being top new followers this week happy   Want this\n",
      "Tweet 869: Share the love: thanks for being top new followers this week happy  Want this?\n",
      "Similarity: 1.00\n",
      "Tweet 428: thanks happy\n",
      "Tweet 1001: .Thanks happy\n",
      "Similarity: 1.00\n",
      "Tweet 524: thanks for the recent follow. Much appreciated happy  Want this\n",
      "Tweet 1141: thanks for the recent follow. Much appreciated happy   Want this ?\n",
      "Similarity: 1.00\n",
      "--------------------------------------------------------------------------------\n",
      "Top 10 most similar tweets using stemming for preprocessing and tfidf for vectorization:\n",
      "Tweet 1001: .Thanks happy\n",
      "Tweet 1874: thanks! happy\n",
      "Similarity: 1.00\n",
      "Tweet 101: Thanks happy\n",
      "Tweet 428: thanks happy\n",
      "Similarity: 1.00\n",
      "Tweet 428: thanks happy\n",
      "Tweet 1001: .Thanks happy\n",
      "Similarity: 1.00\n",
      "Tweet 12: Share the love:thanks for being top new followers this week happy   Want this\n",
      "Tweet 869: Share the love: thanks for being top new followers this week happy  Want this?\n",
      "Similarity: 1.00\n",
      "Tweet 1407: thanks for the recent follow. Much appreciated happy   Want this\n",
      "Tweet 1744: thanks for the recent follow, much appreciated happy   Want this ?\n",
      "Similarity: 1.00\n",
      "Tweet 423: Thanks for being top engaged community members this week happy   Want this\n",
      "Tweet 1113: Thanks for being top engaged community members this week happy  (Want this ?\n",
      "Similarity: 1.00\n",
      "Tweet 1113: Thanks for being top engaged community members this week happy  (Want this ?\n",
      "Tweet 2006: Thanks for being top engaged community members this week happy  Want this ?\n",
      "Similarity: 1.00\n",
      "Tweet 524: thanks for the recent follow. Much appreciated happy  Want this\n",
      "Tweet 2001: thanks for the recent follow, much appreciated happy   Want this\n",
      "Similarity: 1.00\n",
      "Tweet 1744: thanks for the recent follow, much appreciated happy   Want this ?\n",
      "Tweet 1962: thanks for the recent follow. Much appreciated happy  Want this ?\n",
      "Similarity: 1.00\n",
      "Tweet 467: babies unhappy\n",
      "Tweet 820: baby unhappy\n",
      "Similarity: 1.00\n",
      "--------------------------------------------------------------------------------\n",
      "Top 10 most similar tweets using lemmatization for preprocessing and tfidf for vectorization:\n",
      "Tweet 48: Thank you happy\n",
      "Tweet 366: THANK YOU happy\n",
      "Similarity: 1.00\n",
      "Tweet 366: THANK YOU happy\n",
      "Tweet 1043: Thank you! happy\n",
      "Similarity: 1.00\n",
      "Tweet 77: thank you! happy\n",
      "Tweet 546: thank you happy\n",
      "Similarity: 1.00\n",
      "Tweet 12: Share the love:thanks for being top new followers this week happy   Want this\n",
      "Tweet 869: Share the love: thanks for being top new followers this week happy  Want this?\n",
      "Similarity: 1.00\n",
      "Tweet 1605: Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this ?\n",
      "Tweet 1725: Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this\n",
      "Similarity: 1.00\n",
      "Tweet 1415: Thanks for the recent follow Happy to connect happy  have a great Thursday.  Want this ?\n",
      "Tweet 1605: Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this ?\n",
      "Similarity: 1.00\n",
      "Tweet 428: thanks happy\n",
      "Tweet 1001: .Thanks happy\n",
      "Similarity: 1.00\n",
      "Tweet 524: thanks for the recent follow. Much appreciated happy  Want this\n",
      "Tweet 1744: thanks for the recent follow, much appreciated happy   Want this ?\n",
      "Similarity: 1.00\n",
      "Tweet 101: Thanks happy\n",
      "Tweet 428: thanks happy\n",
      "Similarity: 1.00\n",
      "Tweet 1141: thanks for the recent follow. Much appreciated happy   Want this ?\n",
      "Tweet 1407: thanks for the recent follow. Much appreciated happy   Want this\n",
      "Similarity: 1.00\n",
      "--------------------------------------------------------------------------------\n",
      "Top 10 most similar tweets using stemming+misspellings for preprocessing and tfidf for vectorization:\n",
      "Tweet 101: Thanks happy\n",
      "Tweet 428: thanks happy\n",
      "Similarity: 1.00\n",
      "Tweet 1674: thanks for the recent follow. Much appreciated happy  Want this?\n",
      "Tweet 2001: thanks for the recent follow, much appreciated happy   Want this\n",
      "Similarity: 1.00\n",
      "Tweet 1407: thanks for the recent follow. Much appreciated happy   Want this\n",
      "Tweet 1962: thanks for the recent follow. Much appreciated happy  Want this ?\n",
      "Similarity: 1.00\n",
      "Tweet 953: Hello everyone, have a great Thursday! Looking forward to reading your tweets happy  Want this\n",
      "Tweet 974: Hello everyone, have a great Thursday! Looking forward to reading your tweets happy   Want this\n",
      "Similarity: 1.00\n",
      "Tweet 524: thanks for the recent follow. Much appreciated happy  Want this\n",
      "Tweet 1674: thanks for the recent follow. Much appreciated happy  Want this?\n",
      "Similarity: 1.00\n",
      "Tweet 1001: .Thanks happy\n",
      "Tweet 1874: thanks! happy\n",
      "Similarity: 1.00\n",
      "Tweet 974: Hello everyone, have a great Thursday! Looking forward to reading your tweets happy   Want this\n",
      "Tweet 1492: Hello everyone, have a great Thursday! Looking forward to reading your tweets happy  Want this ?\n",
      "Similarity: 1.00\n",
      "Tweet 566: Thanks for the recent follow Happy to connect happy  have a great Thursday.  Want this\n",
      "Tweet 1663: Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this?\n",
      "Similarity: 1.00\n",
      "Tweet 423: Thanks for being top engaged community members this week happy   Want this\n",
      "Tweet 2006: Thanks for being top engaged community members this week happy  Want this ?\n",
      "Similarity: 1.00\n",
      "Tweet 1469: miss you unhappy\n",
      "Tweet 1728: I miss you unhappy\n",
      "Similarity: 1.00\n",
      "--------------------------------------------------------------------------------\n",
      "Top 10 most similar tweets using lemmatization+misspellings for preprocessing and tfidf for vectorization:\n",
      "Tweet 423: Thanks for being top engaged community members this week happy   Want this\n",
      "Tweet 2006: Thanks for being top engaged community members this week happy  Want this ?\n",
      "Similarity: 1.00\n",
      "Tweet 1429: Thanks for the recent follow Happy to connect happy  have a great Thursday. (Want this?\n",
      "Tweet 1663: Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this?\n",
      "Similarity: 1.00\n",
      "Tweet 1415: Thanks for the recent follow Happy to connect happy  have a great Thursday.  Want this ?\n",
      "Tweet 1725: Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this\n",
      "Similarity: 1.00\n",
      "Tweet 566: Thanks for the recent follow Happy to connect happy  have a great Thursday.  Want this\n",
      "Tweet 1605: Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this ?\n",
      "Similarity: 1.00\n",
      "Tweet 12: Share the love:thanks for being top new followers this week happy   Want this\n",
      "Tweet 869: Share the love: thanks for being top new followers this week happy  Want this?\n",
      "Similarity: 1.00\n",
      "Tweet 48: Thank you happy\n",
      "Tweet 77: thank you! happy\n",
      "Similarity: 1.00\n",
      "Tweet 953: Hello everyone, have a great Thursday! Looking forward to reading your tweets happy  Want this\n",
      "Tweet 974: Hello everyone, have a great Thursday! Looking forward to reading your tweets happy   Want this\n",
      "Similarity: 1.00\n",
      "Tweet 366: THANK YOU happy\n",
      "Tweet 1043: Thank you! happy\n",
      "Similarity: 1.00\n",
      "Tweet 974: Hello everyone, have a great Thursday! Looking forward to reading your tweets happy   Want this\n",
      "Tweet 1492: Hello everyone, have a great Thursday! Looking forward to reading your tweets happy  Want this ?\n",
      "Similarity: 1.00\n",
      "Tweet 101: Thanks happy\n",
      "Tweet 428: thanks happy\n",
      "Similarity: 1.00\n",
      "--------------------------------------------------------------------------------\n",
      "Top 10 most similar tweets using just_tokenization for preprocessing and bigrams for vectorization:\n",
      "Tweet 953: Hello everyone, have a great Thursday! Looking forward to reading your tweets happy  Want this\n",
      "Tweet 974: Hello everyone, have a great Thursday! Looking forward to reading your tweets happy   Want this\n",
      "Similarity: 1.00\n",
      "Tweet 974: Hello everyone, have a great Thursday! Looking forward to reading your tweets happy   Want this\n",
      "Tweet 1492: Hello everyone, have a great Thursday! Looking forward to reading your tweets happy  Want this ?\n",
      "Similarity: 1.00\n",
      "Tweet 1113: Thanks for being top engaged community members this week happy  (Want this ?\n",
      "Tweet 2006: Thanks for being top engaged community members this week happy  Want this ?\n",
      "Similarity: 1.00\n",
      "Tweet 423: Thanks for being top engaged community members this week happy   Want this\n",
      "Tweet 1917: Thanks for being top engaged community members this week happy  Want this\n",
      "Similarity: 1.00\n",
      "Tweet 632: pake unhappy\n",
      "Tweet 1844: pake 3 unhappy\n",
      "Similarity: 1.00\n",
      "Tweet 101: Thanks happy\n",
      "Tweet 428: thanks happy\n",
      "Similarity: 1.00\n",
      "Tweet 428: thanks happy\n",
      "Tweet 1001: .Thanks happy\n",
      "Similarity: 1.00\n",
      "Tweet 1001: .Thanks happy\n",
      "Tweet 1874: thanks! happy\n",
      "Similarity: 1.00\n",
      "Tweet 1136: When will you notice me? unhappy\n",
      "Tweet 1934: when will you notice me? unhappy\n",
      "Similarity: 1.00\n",
      "Tweet 546: thank you happy\n",
      "Tweet 1043: Thank you! happy\n",
      "Similarity: 1.00\n",
      "--------------------------------------------------------------------------------\n",
      "Top 10 most similar tweets using stemming for preprocessing and bigrams for vectorization:\n",
      "Tweet 953: Hello everyone, have a great Thursday! Looking forward to reading your tweets happy  Want this\n",
      "Tweet 974: Hello everyone, have a great Thursday! Looking forward to reading your tweets happy   Want this\n",
      "Similarity: 1.00\n",
      "Tweet 974: Hello everyone, have a great Thursday! Looking forward to reading your tweets happy   Want this\n",
      "Tweet 1492: Hello everyone, have a great Thursday! Looking forward to reading your tweets happy  Want this ?\n",
      "Similarity: 1.00\n",
      "Tweet 423: Thanks for being top engaged community members this week happy   Want this\n",
      "Tweet 1917: Thanks for being top engaged community members this week happy  Want this\n",
      "Similarity: 1.00\n",
      "Tweet 1113: Thanks for being top engaged community members this week happy  (Want this ?\n",
      "Tweet 2006: Thanks for being top engaged community members this week happy  Want this ?\n",
      "Similarity: 1.00\n",
      "Tweet 428: thanks happy\n",
      "Tweet 1001: .Thanks happy\n",
      "Similarity: 1.00\n",
      "Tweet 101: Thanks happy\n",
      "Tweet 428: thanks happy\n",
      "Similarity: 1.00\n",
      "Tweet 632: pake unhappy\n",
      "Tweet 1844: pake 3 unhappy\n",
      "Similarity: 1.00\n",
      "Tweet 1136: When will you notice me? unhappy\n",
      "Tweet 1934: when will you notice me? unhappy\n",
      "Similarity: 1.00\n",
      "Tweet 1001: .Thanks happy\n",
      "Tweet 1874: thanks! happy\n",
      "Similarity: 1.00\n",
      "Tweet 467: babies unhappy\n",
      "Tweet 820: baby unhappy\n",
      "Similarity: 1.00\n",
      "--------------------------------------------------------------------------------\n",
      "Top 10 most similar tweets using lemmatization for preprocessing and bigrams for vectorization:\n",
      "Tweet 974: Hello everyone, have a great Thursday! Looking forward to reading your tweets happy   Want this\n",
      "Tweet 1492: Hello everyone, have a great Thursday! Looking forward to reading your tweets happy  Want this ?\n",
      "Similarity: 1.00\n",
      "Tweet 953: Hello everyone, have a great Thursday! Looking forward to reading your tweets happy  Want this\n",
      "Tweet 974: Hello everyone, have a great Thursday! Looking forward to reading your tweets happy   Want this\n",
      "Similarity: 1.00\n",
      "Tweet 1917: Thanks for being top engaged community members this week happy  Want this\n",
      "Tweet 2006: Thanks for being top engaged community members this week happy  Want this ?\n",
      "Similarity: 1.00\n",
      "Tweet 423: Thanks for being top engaged community members this week happy   Want this\n",
      "Tweet 1113: Thanks for being top engaged community members this week happy  (Want this ?\n",
      "Similarity: 1.00\n",
      "Tweet 101: Thanks happy\n",
      "Tweet 1874: thanks! happy\n",
      "Similarity: 1.00\n",
      "Tweet 1136: When will you notice me? unhappy\n",
      "Tweet 1934: when will you notice me? unhappy\n",
      "Similarity: 1.00\n",
      "Tweet 428: thanks happy\n",
      "Tweet 1001: .Thanks happy\n",
      "Similarity: 1.00\n",
      "Tweet 632: pake unhappy\n",
      "Tweet 1844: pake 3 unhappy\n",
      "Similarity: 1.00\n",
      "Tweet 467: babies unhappy\n",
      "Tweet 820: baby unhappy\n",
      "Similarity: 1.00\n",
      "Tweet 48: Thank you happy\n",
      "Tweet 546: thank you happy\n",
      "Similarity: 1.00\n",
      "--------------------------------------------------------------------------------\n",
      "Top 10 most similar tweets using stemming+misspellings for preprocessing and bigrams for vectorization:\n",
      "Tweet 974: Hello everyone, have a great Thursday! Looking forward to reading your tweets happy   Want this\n",
      "Tweet 1492: Hello everyone, have a great Thursday! Looking forward to reading your tweets happy  Want this ?\n",
      "Similarity: 1.00\n",
      "Tweet 953: Hello everyone, have a great Thursday! Looking forward to reading your tweets happy  Want this\n",
      "Tweet 974: Hello everyone, have a great Thursday! Looking forward to reading your tweets happy   Want this\n",
      "Similarity: 1.00\n",
      "Tweet 423: Thanks for being top engaged community members this week happy   Want this\n",
      "Tweet 2006: Thanks for being top engaged community members this week happy  Want this ?\n",
      "Similarity: 1.00\n",
      "Tweet 101: Thanks happy\n",
      "Tweet 1874: thanks! happy\n",
      "Similarity: 1.00\n",
      "Tweet 428: thanks happy\n",
      "Tweet 1001: .Thanks happy\n",
      "Similarity: 1.00\n",
      "Tweet 467: babies unhappy\n",
      "Tweet 820: baby unhappy\n",
      "Similarity: 1.00\n",
      "Tweet 632: pake unhappy\n",
      "Tweet 1844: pake 3 unhappy\n",
      "Similarity: 1.00\n",
      "Tweet 1136: When will you notice me? unhappy\n",
      "Tweet 1934: when will you notice me? unhappy\n",
      "Similarity: 1.00\n",
      "Tweet 546: thank you happy\n",
      "Tweet 1043: Thank you! happy\n",
      "Similarity: 1.00\n",
      "Tweet 366: THANK YOU happy\n",
      "Tweet 546: thank you happy\n",
      "Similarity: 1.00\n",
      "--------------------------------------------------------------------------------\n",
      "Top 10 most similar tweets using lemmatization+misspellings for preprocessing and bigrams for vectorization:\n",
      "Tweet 974: Hello everyone, have a great Thursday! Looking forward to reading your tweets happy   Want this\n",
      "Tweet 1492: Hello everyone, have a great Thursday! Looking forward to reading your tweets happy  Want this ?\n",
      "Similarity: 1.00\n",
      "Tweet 953: Hello everyone, have a great Thursday! Looking forward to reading your tweets happy  Want this\n",
      "Tweet 974: Hello everyone, have a great Thursday! Looking forward to reading your tweets happy   Want this\n",
      "Similarity: 1.00\n",
      "Tweet 423: Thanks for being top engaged community members this week happy   Want this\n",
      "Tweet 1113: Thanks for being top engaged community members this week happy  (Want this ?\n",
      "Similarity: 1.00\n",
      "Tweet 1113: Thanks for being top engaged community members this week happy  (Want this ?\n",
      "Tweet 1917: Thanks for being top engaged community members this week happy  Want this\n",
      "Similarity: 1.00\n",
      "Tweet 1917: Thanks for being top engaged community members this week happy  Want this\n",
      "Tweet 2006: Thanks for being top engaged community members this week happy  Want this ?\n",
      "Similarity: 1.00\n",
      "Tweet 101: Thanks happy\n",
      "Tweet 1874: thanks! happy\n",
      "Similarity: 1.00\n",
      "Tweet 1136: When will you notice me? unhappy\n",
      "Tweet 1934: when will you notice me? unhappy\n",
      "Similarity: 1.00\n",
      "Tweet 467: babies unhappy\n",
      "Tweet 820: baby unhappy\n",
      "Similarity: 1.00\n",
      "Tweet 632: pake unhappy\n",
      "Tweet 1844: pake 3 unhappy\n",
      "Similarity: 1.00\n",
      "Tweet 1469: miss you unhappy\n",
      "Tweet 1728: I miss you unhappy\n",
      "Similarity: 1.00\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Calculate the top 10 most similar tweets for each combination of vectorization and preprocessing technique\n",
    "for v in vectorization_techniques:\n",
    "    for p in preprocessing_techniques:\n",
    "        print(f'Top 10 most similar tweets using {p} for preprocessing and {v} for vectorization:')\n",
    "        vectorizer = results.loc[p, v]\n",
    "        X_train_vectorized = vectorizer.transform(X_train)\n",
    "        top = find_similar_tweets(X_train_vectorized, normalized=(v == 'tfidf'))\n",
    "        for i in top.index:\n",
    "            idx_1 = top['tweet1'][i]\n",
    "            idx_2 = top['tweet2'][i]\n",
    "            print(f'Tweet {idx_1}:', X_train.iloc[idx_1])\n",
    "            print(f'Tweet {idx_2}:', X_train.iloc[idx_2])\n",
    "            print(f\"Similarity: {top['similarity'][i]:.2f}\")\n",
    "        print('-' * 80)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Machine learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try different algorithms and the different approaches that we prepared before to solve the classification task – sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the models we will be using\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "classifiers = [KNeighborsClassifier(), LogisticRegression(), RandomForestClassifier(random_state=1337), SVC(), ComplementNB()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'KNeighborsClassifier': KNeighborsClassifier(),\n",
       " 'LogisticRegression': LogisticRegression(),\n",
       " 'RandomForestClassifier': RandomForestClassifier(random_state=1337),\n",
       " 'SVC': SVC(),\n",
       " 'ComplementNB': ComplementNB()}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert classifiers to dictionary\n",
    "classifiers_dict = {type(c).__name__: c for c in classifiers}\n",
    "classifiers_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe to store results\n",
    "accuracy = pd.DataFrame(columns=vectorization_techniques, index=preprocessing_techniques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier\n",
      "                              binary word_count     tfidf   bigrams\n",
      "just_tokenization           0.609982   0.621072  0.759704  0.534196\n",
      "stemming                    0.600739   0.617375  0.759704  0.530499\n",
      "lemmatization               0.608133   0.622921   0.76525  0.537893\n",
      "stemming+misspellings       0.604436   0.617375  0.748614  0.528651\n",
      "lemmatization+misspellings  0.609982   0.619224  0.759704  0.537893\n",
      "\n",
      "LogisticRegression\n",
      "                              binary word_count     tfidf   bigrams\n",
      "just_tokenization           0.922366   0.913124  0.913124  0.911275\n",
      "stemming                    0.916821   0.911275  0.909427  0.909427\n",
      "lemmatization               0.911275   0.902033  0.913124   0.90573\n",
      "stemming+misspellings       0.918669   0.907579  0.911275  0.909427\n",
      "lemmatization+misspellings  0.909427   0.900185  0.911275   0.90573\n",
      "\n",
      "RandomForestClassifier\n",
      "                              binary word_count     tfidf   bigrams\n",
      "just_tokenization           0.911275   0.924214  0.916821  0.907579\n",
      "stemming                    0.911275   0.916821  0.922366  0.909427\n",
      "lemmatization               0.914972   0.911275  0.924214  0.907579\n",
      "stemming+misspellings       0.909427   0.913124  0.916821  0.898336\n",
      "lemmatization+misspellings   0.90573   0.907579  0.913124  0.896488\n",
      "\n",
      "SVC\n",
      "                              binary word_count     tfidf   bigrams\n",
      "just_tokenization           0.907579    0.90573  0.924214  0.896488\n",
      "stemming                    0.911275   0.903882  0.924214  0.898336\n",
      "lemmatization               0.909427    0.90573  0.924214  0.896488\n",
      "stemming+misspellings       0.907579   0.902033  0.920518  0.898336\n",
      "lemmatization+misspellings  0.903882   0.903882  0.922366   0.89464\n",
      "\n",
      "ComplementNB\n",
      "                              binary word_count     tfidf   bigrams\n",
      "just_tokenization           0.931608   0.931608  0.922366  0.924214\n",
      "stemming                    0.935305   0.937153  0.913124  0.927911\n",
      "lemmatization                0.92976   0.933457  0.914972  0.924214\n",
      "stemming+misspellings       0.927911   0.935305  0.909427  0.922366\n",
      "lemmatization+misspellings  0.924214   0.927911  0.909427  0.916821\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for classifier in classifiers_dict:\n",
    "    print(classifier)\n",
    "    clf = classifiers_dict[classifier]\n",
    "    for v in vectorization_techniques:\n",
    "        for p in preprocessing_techniques:\n",
    "            vectorizer = results.loc[p, v]\n",
    "            X_train_vectorized = vectorizer.transform(X_train)\n",
    "            X_test_vectorized = vectorizer.transform(X_test)\n",
    "            clf.fit(X_train_vectorized, y_train)\n",
    "            y_pred = clf.predict(X_test_vectorized)\n",
    "            acc_score = accuracy_score(y_test, y_pred)\n",
    "            accuracy.loc[p, v] = acc_score\n",
    "    print(accuracy)\n",
    "    print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show additional metrics for the best performing algorithm and approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.89      0.95      0.92       173\n",
      "           0       0.97      0.96      0.96       202\n",
      "           1       0.95      0.89      0.92       166\n",
      "\n",
      "    accuracy                           0.94       541\n",
      "   macro avg       0.94      0.94      0.94       541\n",
      "weighted avg       0.94      0.94      0.94       541\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectorizer = results.loc['stemming', 'word_count']\n",
    "X_train_vectorized = vectorizer.transform(X_train)\n",
    "X_test_vectorized = vectorizer.transform(X_test)\n",
    "\n",
    "best_classifier = classifiers_dict['ComplementNB']\n",
    "best_classifier.fit(X_train_vectorized, y_train)\n",
    "y_pred = best_classifier.predict(X_test_vectorized)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "42AI-mfarhi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
